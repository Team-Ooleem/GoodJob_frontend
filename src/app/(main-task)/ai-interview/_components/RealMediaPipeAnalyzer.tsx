'use client';

interface DetectionResult {
    type: 'posture' | 'eye_contact' | 'smile' | 'gesture' | 'confidence' | 'attention' | 'stress';
    message: string;
    level: 'good' | 'warning' | 'excellent';
    confidence: number;
    landmarks: any;
    metrics: any;
}

type Presence = 'good' | 'average' | 'needs_improvement';
type LevelAgg = 'ok' | 'info' | 'warning' | 'critical';

// ì„œë²„ì— ë³´ë‚¼ ë¬¸í•­ ì§‘ê³„ í˜ì´ë¡œë“œ (Nestì˜ VisualAggregateDtoì™€ 1:1)
export type VisualAggregatePayload = {
    sample_count: number;

    confidence_mean: number | null;
    confidence_max: number | null;
    smile_mean: number | null;
    smile_max: number | null;

    // ì¶”ê°€ ì§€í‘œ(í‰ê· )
    eye_contact_mean: number | null; // 0~1
    blink_mean: number | null; // 0~1 (ê¹œë¹¡ì„ í™•ë¥  í”„ë¡ì‹œ)
    gaze_stability: number | null; // 0~1 (ë†’ì„ìˆ˜ë¡ ì•ˆì •)

    presence_good: number;
    presence_average: number;
    presence_needs_improvement: number;

    level_ok: number;
    level_info: number;
    level_warning: number;
    level_critical: number;

    left_eye_x_mean: number | null;
    left_eye_y_mean: number | null;
    right_eye_x_mean: number | null;
    right_eye_y_mean: number | null;
    nose_x_mean: number | null;
    nose_y_mean: number | null;

    started_at_ms: number | null;
    ended_at_ms: number | null;

    // (ì„ íƒ) í´ë¼ì´ì–¸íŠ¸ í¸ì˜ë¥¼ ìœ„í•œ ë©”íƒ€
    _questionId?: string;
    _orderNo?: number;
    _text?: string;
};

// ë‚´ë¶€ ë²„í¼ì— ì €ì¥í•  ìƒ˜í”Œ ìŠ¤í‚¤ë§ˆ(ê²½ëŸ‰)
type VisualSampleLite = {
    timestamp: string; // ISO
    detection: {
        confidenceScore?: number;
        smileIntensity?: number;
        eyeContact?: number; // 0~1
        blinkProb?: number; // 0~1
        overallPresence?: Presence;
        level?: LevelAgg;
        landmarks: {
            leftEye?: { x: number; y: number };
            rightEye?: { x: number; y: number };
            nose?: { x: number; y: number };
        };
    };
};

export class RealMediaPipeAnalyzer {
    private videoRef: React.RefObject<HTMLVideoElement>;
    private canvasRef: React.RefObject<HTMLCanvasElement>;
    private onDetection: (data: DetectionResult) => void;

    private isAnalyzing = false;
    private animationFrameId: number | null = null;
    private lastLogTime = 0;
    private logInterval = 10000; // 10ì´ˆë§ˆë‹¤ ë¶„ì„ ê²°ê³¼ ë¡œê·¸ ì¶œë ¥
    private analysisInterval = 2000; // 2ì´ˆë§ˆë‹¤ ë¶„ì„ ì‹¤í–‰
    private lastAnalysisTime = 0;

    // MediaPipe Tasks Vision
    private faceLandmarker: any = null;
    private vision: any = null;
    private isInitialized = false;

    // ìƒíƒœ
    private lastFaceResults: any = null;
    private lastDetectionTime = 0;
    private lastAnalysisLogTime = 0;
    private hasLoggedInitialData = false;
    
    // ìº˜ë¦¬ë¸Œë ˆì´ì…˜ êµ¬ì„±(ë¡œì»¬ ì €ì¥ ê¸°ë°˜)
    private calib: {
        baselineConfidence?: number | null;
        baselineSmile?: number | null;
        presenceGoodCut: number;
        presenceNeedsCut: number;
        confWarnThreshold: number;
        confGoodThreshold: number;
        attentionWarnThreshold: number;
        stressWarnThreshold: number;
    } | null = null;
    // ë°˜ë³µ ê²½ê³  ì™„í™”ìš© ì¿¨ë‹¤ìš´
    private lastFeedbackType: string | null = null;
    private lastFeedbackTime = 0;
    private readonly feedbackCooldownMs = 5000; // ë™ì¼ íƒ€ì… 5ì´ˆ ì¿¨ë‹¤ìš´
    // ì§€í‘œ ìŠ¤ë¬´ë”©ìš© ë²„í¼
    private prevMetrics: {
        confidence: number;
        stress: number;
        attention: number;
        engagement: number;
        eyeContact: number;
        smile: number;
        nervousness: number;
        composure: number;
    } | null = null;
    private readonly smoothAlpha = 0.3; // EMA ê³„ìˆ˜(0.0~1.0)

    // â–¼ ì¶”ê°€: ë¬¸í•­ ë²„í¼ë§/ë©”íƒ€
    private currentQuestionId: string | null = null;
    private currentOrderNo: number | undefined;
    private currentQuestionText: string | undefined;
    private sampleBuffer: VisualSampleLite[] = [];
    private questionStartedAt: number | null = null;

    constructor(
        videoRef: React.RefObject<HTMLVideoElement>,
        canvasRef: React.RefObject<HTMLCanvasElement>,
        onDetection: (data: DetectionResult) => void,
    ) {
        this.videoRef = videoRef;
        this.canvasRef = canvasRef;
        this.onDetection = onDetection;
    }

    private clamp01(v: number) {
        return Math.max(0, Math.min(1, v));
    }

    private loadCalibration() {
        try {
            if (typeof window === 'undefined') return;
            const raw = localStorage.getItem('aiInterviewCalibration');
            if (!raw) {
                this.calib = {
                    presenceGoodCut: 0.6,
                    presenceNeedsCut: 0.4,
                    confWarnThreshold: 0.3,
                    confGoodThreshold: 0.65,
                    attentionWarnThreshold: 0.45,
                    stressWarnThreshold: 0.6,
                };
                return;
            }
            const data = JSON.parse(raw) as any;
            const v = data?.visual as VisualAggregatePayload | undefined;
            const baselineConfidence = typeof v?.confidence_mean === 'number' ? v.confidence_mean : null;
            const baselineSmile = typeof v?.smile_mean === 'number' ? v.smile_mean : null;

            const baseConf = baselineConfidence ?? 0.6;
            const confWarn = Math.max(0.25, baseConf - 0.15);
            const confGood = Math.min(0.9, baseConf + 0.08);
            const presGood = this.clamp01(0.6 + (baseConf - 0.6) * 0.5);
            const presNeeds = this.clamp01(presGood - 0.2);

            this.calib = {
                baselineConfidence,
                baselineSmile,
                presenceGoodCut: presGood,
                presenceNeedsCut: presNeeds,
                confWarnThreshold: confWarn,
                confGoodThreshold: confGood,
                attentionWarnThreshold: 0.45,
                stressWarnThreshold: 0.6,
            };
            // console.debug('[Calibration] Loaded visual calibration:', this.calib);
        } catch (e) {
            // console.warn('[Calibration] Failed to load calibration. Using defaults.', e);
            this.calib = {
                presenceGoodCut: 0.6,
                presenceNeedsCut: 0.4,
                confWarnThreshold: 0.3,
                confGoodThreshold: 0.65,
                attentionWarnThreshold: 0.45,
                stressWarnThreshold: 0.6,
            };
        }
    }

    // ===========================
    // Public: ë¬¸í•­ ì‹œì‘/ì¢…ë£Œ API
    // ===========================
    public startQuestion(questionId: string, opts?: { orderNo?: number; text?: string }) {
        this.currentQuestionId = questionId;
        this.currentOrderNo = opts?.orderNo;
        this.currentQuestionText = opts?.text;
        this.sampleBuffer = [];
        this.questionStartedAt = Date.now();
        if (questionId === 'calibration') {
            console.log('[Calibration] ë¹„ë””ì˜¤ ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ì‹œì‘');
        }
        // ìµœì´ˆ í•œë‘ í”„ë ˆì„ì€ ë¶„ì„ ì´ì „ì¼ ìˆ˜ ìˆì–´ ìƒ˜í”Œ ì—†ì„ ìˆ˜ ìˆìŒ(OK)
        // ë³„ë„ ë¡œê·¸ë§Œ
        // console.log('[Mediapipe] startQuestion:', questionId, opts);
    }

    public endQuestion(): VisualAggregatePayload {
        const payload = this.computeAggregate(this.sampleBuffer);
        // ë©”íƒ€ í¬í•¨
        payload._questionId = this.currentQuestionId ?? undefined;
        payload._orderNo = this.currentOrderNo;
        payload._text = this.currentQuestionText;

        // ë‹¤ìŒ ë¬¸í•­ ìœ„í•´ ë¦¬ì…‹
        this.currentQuestionId = null;
        this.currentOrderNo = undefined;
        this.currentQuestionText = undefined;
        this.sampleBuffer = [];
        this.questionStartedAt = null;
        if (payload._questionId === 'calibration') {
            console.log('[Calibration] ë¹„ë””ì˜¤ ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ì¢…ë£Œ - samples:', payload.sample_count);
        }

        return payload;
    }

    // ===========================
    // MediaPipe ì´ˆê¸°í™”/ë£¨í”„
    // ===========================
    public async initializeModels() {
        if (this.isInitialized) {
            console.log('âš ï¸ MediaPipe ëª¨ë¸ì´ ì´ë¯¸ ì´ˆê¸°í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ì¤‘ë³µ ì‹¤í–‰ ë°©ì§€.');
            return;
        }
        try {
            console.log('ğŸš€ MediaPipe Tasks Vision ëª¨ë¸ ì´ˆê¸°í™” ì‹œì‘...');
            const { FaceLandmarker, FilesetResolver } = await import('@mediapipe/tasks-vision');
            this.vision = await FilesetResolver.forVisionTasks('/mediapipe/wasm');
            this.faceLandmarker = await FaceLandmarker.createFromOptions(this.vision, {
                baseOptions: { modelAssetPath: '/mediapipe/face_landmarker.task' },
                outputFaceBlendshapes: true,
                outputFacialTransformationMatrixes: false,
                runningMode: 'VIDEO',
                numFaces: 1,
            });
            this.isInitialized = true;
            console.log('âœ… MediaPipe Tasks Vision ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ');
            // ì´ˆê¸°í™” í›„ ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ë¡œë“œ
            this.loadCalibration();
        } catch (error) {
            console.error('âŒ MediaPipe Tasks Vision ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨:', error);
            this.isInitialized = false;
            throw error;
        }
    }

    private analyzeFaceResults(results: any) {
        const currentTime = Date.now();

        // 10ì´ˆë§ˆë‹¤ ìš”ì•½ ë¡œê·¸ (ë¬¸í•­ ìˆ˜ì§‘ ì¤‘ì¼ ë•Œë§Œ)
        if (this.currentQuestionId && currentTime - this.lastAnalysisLogTime >= this.logInterval) {
            console.log('ğŸ‘ï¸ Face Landmarker ë¶„ì„ ê²°ê³¼ (10ì´ˆë§ˆë‹¤):', {
                timestamp: new Date().toISOString(),
                hasLandmarks: !!results.faceLandmarks,
                facesCount: results.faceLandmarks?.length || 0,
                hasBlendshapes: !!results.faceBlendshapes,
                blendshapesCount: results.faceBlendshapes?.length || 0,
                firstFaceLandmarks: results.faceLandmarks?.[0]
                    ? results.faceLandmarks[0].slice(0, 5)
                    : null,
            });
            this.lastAnalysisLogTime = currentTime;
        }

        if (!results.faceLandmarks || results.faceLandmarks.length === 0) return;

        const faceLandmarks = results.faceLandmarks[0];
        const faceBlendshapes = results.faceBlendshapes?.[0];

        const leftEyeCenter = faceLandmarks[159];
        const rightEyeCenter = faceLandmarks[386];
        const noseTip = faceLandmarks[1];

        const rawMetrics = this.calculateInterviewMetrics(faceBlendshapes, faceLandmarks);
        // ì§€í‘œ ìŠ¤ë¬´ë”©(EMA) ì ìš©
        const mPrev = this.prevMetrics;
        const a = this.smoothAlpha;
        const interviewMetrics = mPrev
            ? {
                  confidence: a * rawMetrics.confidence + (1 - a) * mPrev.confidence,
                  stress: a * rawMetrics.stress + (1 - a) * mPrev.stress,
                  attention: a * rawMetrics.attention + (1 - a) * mPrev.attention,
                  engagement: a * rawMetrics.engagement + (1 - a) * mPrev.engagement,
                  eyeContact: a * rawMetrics.eyeContact + (1 - a) * mPrev.eyeContact,
                  smile: a * rawMetrics.smile + (1 - a) * mPrev.smile,
                  nervousness: a * rawMetrics.nervousness + (1 - a) * mPrev.nervousness,
                  composure: a * rawMetrics.composure + (1 - a) * mPrev.composure,
              }
            : rawMetrics;
        this.prevMetrics = interviewMetrics;

        // â–¼ ì‹¤ì‹œê°„ í”¼ë“œë°±: ë‹µë³€(ë¬¸í•­) ì§„í–‰ ì¤‘ì¼ ë•Œë§Œ í‘œì‹œ/ë°œìƒ
        if (this.currentQuestionId) {
            this.generateInterviewFeedback(interviewMetrics, leftEyeCenter, rightEyeCenter, noseTip);
        }

        // â–¼ ì¶”ê°€: 5ì´ˆ ê°„ê²© ë¶„ì„ì‹œ **ë²„í¼ì— ìƒ˜í”Œ ê¸°ë¡** (ë¬¸í•­ ì§„í–‰ì¤‘ì¼ ë•Œë§Œ)
        if (this.currentQuestionId) {
            // ì¡´ì¬ê°(presence): ìì‹ ê°/ì§‘ì¤‘/ê¸´ì¥ì™„í™”(1-stress) ê°€ì¤‘í•©
            const presenceScore =
                0.5 * interviewMetrics.confidence +
                0.3 * interviewMetrics.attention +
                0.2 * (1 - interviewMetrics.stress);
            const goodCut = this.calib?.presenceGoodCut ?? 0.6;
            const needsCut = this.calib?.presenceNeedsCut ?? 0.4;
            const presence: Presence =
                presenceScore >= goodCut
                    ? 'good'
                    : presenceScore < needsCut
                      ? 'needs_improvement'
                      : 'average';

            // level ë§¤í•‘: ìŠ¤íŠ¸ë ˆìŠ¤ê°€ ë†’ì€ êµ¬ê°„ë§Œ 'warning'
            const level: LevelAgg =
                interviewMetrics.stress > (this.calib?.stressWarnThreshold ?? 0.6)
                    ? 'warning'
                    : 'ok';

            const sample: VisualSampleLite = {
                timestamp: new Date().toISOString(),
                detection: {
                    confidenceScore: interviewMetrics.confidence,
                    smileIntensity: interviewMetrics.smile,
                    eyeContact: interviewMetrics.eyeContact,
                    // ê¹œë¹¡ì„ í™•ë¥ : attention ì‚°ì‹ì— í¬í•¨ë˜ë‚˜, ë³„ë„ ë³´ê´€ì„ ìœ„í•´ rough ì¶”ì •
                    // calculateInterviewMetrics ë‚´ë¶€ blinkë¥¼ ì§ì ‘ ì‚¬ìš©í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ, attentionì—ì„œ ì¶”ì • ë¶ˆê°€.
                    // ì—¬ê¸°ì„œëŠ” ì´ì „ ë‹¨ê³„ rawMetricsë¥¼ ì‚¬ìš©í•˜ë„ë¡ calculateInterviewMetricsë¥¼ í™•ì¥(ì•„ë˜ ì°¸ì¡°).
                    blinkProb: (this as any)._lastBlinkProb ?? undefined,
                    overallPresence: presence,
                    level,
                    landmarks: {
                        leftEye: { x: leftEyeCenter.x, y: leftEyeCenter.y },
                        rightEye: { x: rightEyeCenter.x, y: rightEyeCenter.y },
                        nose: { x: noseTip.x, y: noseTip.y },
                    },
                },
            };
            this.sampleBuffer.push(sample);
        }
    }

    private calculateInterviewMetrics(faceBlendshapes: any, faceLandmarks: any) {
        const metrics = {
            confidence: 0,
            stress: 0,
            attention: 0,
            engagement: 0,
            eyeContact: 0,
            smile: 0,
            nervousness: 0,
            composure: 0,
        };
        if (!faceBlendshapes?.categories) return metrics;

        const blendshapes = faceBlendshapes.categories;

        const smileLeft =
            blendshapes.find((c: any) => c.categoryName === 'mouthSmileLeft')?.score || 0;
        const smileRight =
            blendshapes.find((c: any) => c.categoryName === 'mouthSmileRight')?.score || 0;
        const browInnerUp =
            blendshapes.find((c: any) => c.categoryName === 'browInnerUp')?.score || 0;
        const browOuterUpLeft =
            blendshapes.find((c: any) => c.categoryName === 'browOuterUpLeft')?.score || 0;
        const browOuterUpRight =
            blendshapes.find((c: any) => c.categoryName === 'browOuterUpRight')?.score || 0;
        const eyeLookOutLeft =
            blendshapes.find((c: any) => c.categoryName === 'eyeLookOutLeft')?.score || 0;
        const eyeLookOutRight =
            blendshapes.find((c: any) => c.categoryName === 'eyeLookOutRight')?.score || 0;
        const mouthOpen = blendshapes.find((c: any) => c.categoryName === 'jawOpen')?.score || 0;
        const eyeWideLeft = blendshapes.find((c: any) => c.categoryName === 'eyeWideLeft')?.score || 0;
        const eyeWideRight = blendshapes.find((c: any) => c.categoryName === 'eyeWideRight')?.score || 0;

        metrics.smile = (smileLeft + smileRight) / 2;
        // ìì‹ ê°: ê³¼ë„í•œ ë†€ëŒ/ê¸´ì¥(eyeWide, browInnerUpâ†‘)ì€ ê°ì , ì ë‹¹í•œ ë¯¸ì†Œì™€ ì‹œì„  ê³ ì •ì€ ê°€ì 
        // browOuterUp(ì•½ê°„ì˜ ê°œë°©ê°)ë„ ì†Œí­ ê°€ì 
        const surprise = (eyeWideLeft + eyeWideRight + browInnerUp) / 3;
        metrics.confidence =
            0.45 * metrics.smile +
            0.35 * Math.max(0, 1 - surprise) +
            0.2 * ((browOuterUpLeft + browOuterUpRight) / 2);

        const browDownLeft =
            blendshapes.find((c: any) => c.categoryName === 'browDownLeft')?.score || 0;
        const browDownRight =
            blendshapes.find((c: any) => c.categoryName === 'browDownRight')?.score || 0;
        const mouthPressLeft =
            blendshapes.find((c: any) => c.categoryName === 'mouthPressLeft')?.score || 0;
        const mouthPressRight =
            blendshapes.find((c: any) => c.categoryName === 'mouthPressRight')?.score || 0;

        // ìŠ¤íŠ¸ë ˆìŠ¤: ì°¡ê·¸ë¦¼/ì… ê½‰ ë‹¤ë¬¼ê¸° + ëˆˆ í¬ê²Œ ëœ¸ì„ ì¢…í•©
        metrics.stress =
            (browDownLeft +
                browDownRight +
                mouthPressLeft +
                mouthPressRight +
                eyeWideLeft +
                eyeWideRight) /
            6;
        metrics.nervousness = metrics.stress;

        const eyeBlinkLeft =
            blendshapes.find((c: any) => c.categoryName === 'eyeBlinkLeft')?.score || 0;
        const eyeBlinkRight =
            blendshapes.find((c: any) => c.categoryName === 'eyeBlinkRight')?.score || 0;
        const eyeLookDownLeft =
            blendshapes.find((c: any) => c.categoryName === 'eyeLookDownLeft')?.score || 0;
        const eyeLookDownRight =
            blendshapes.find((c: any) => c.categoryName === 'eyeLookDownRight')?.score || 0;

        const blink = (eyeBlinkLeft + eyeBlinkRight) / 2;
        const gazeAway =
            (eyeLookDownLeft + eyeLookDownRight + eyeLookOutLeft + eyeLookOutRight) / 4;
        metrics.eyeContact = Math.max(0, 1 - gazeAway);
        metrics.attention = Math.max(0, Math.min(1, 0.6 * metrics.eyeContact + 0.4 * (1 - blink)));
        // ì™¸ë¶€ì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•˜ë„ë¡ ë§ˆì§€ë§‰ blinkë¥¼ ë³´ê´€
        (this as any)._lastBlinkProb = blink;

        const cheekPuff = blendshapes.find((c: any) => c.categoryName === 'cheekPuff')?.score || 0;
        // ì°¸ì—¬ë„: ì‹œì„ /ì§‘ì¤‘ + ì ë‹¹í•œ ë¯¸ì†Œ + ë°œí™”(ì…ì—´ë¦¼) ì‹ í˜¸
        metrics.engagement = (metrics.attention + metrics.smile + mouthOpen) / 3;
        metrics.composure = 1 - metrics.stress;

        return metrics;
    }

    private generateInterviewFeedback(metrics: any, leftEye: any, rightEye: any, nose: any) {
        const landmarks = {
            leftEye: { x: leftEye.x, y: leftEye.y },
            rightEye: { x: rightEye.x, y: rightEye.y },
            nose: { x: nose.x, y: nose.y },
        };

        const now = Date.now();
        const emit = (type: DetectionResult['type'], payload: DetectionResult) => {
            if (this.lastFeedbackType === type && now - this.lastFeedbackTime < this.feedbackCooldownMs) {
                return; // ë™ì¼ ìœ í˜• ë©”ì‹œì§€ ìŠ¤íŒ¸ ë°©ì§€
            }
            this.lastFeedbackType = type;
            this.lastFeedbackTime = now;
            this.onDetection(payload);
        };

        const confWarnTh = this.calib?.confWarnThreshold ?? 0.3;
        const confGoodTh = this.calib?.confGoodThreshold ?? 0.65;
        const attWarnTh = this.calib?.attentionWarnThreshold ?? 0.45;
        const stressWarnTh = this.calib?.stressWarnThreshold ?? 0.6;

        // 1) ìŠ¤íŠ¸ë ˆìŠ¤ ë†’ìŒ
        if (metrics.stress > stressWarnTh) {
            emit('stress', {
                type: 'stress',
                message: 'ê¸´ì¥ì„ í’€ê³  í¸ì•ˆí•œ ë§ˆìŒìœ¼ë¡œ ë‹µë³€í•´ë³´ì„¸ìš”',
                level: 'warning',
                confidence: 1 - metrics.stress,
                landmarks,
                metrics: {
                    stressLevel: metrics.stress,
                    nervousness: metrics.nervousness,
                    relaxationNeeded: true,
                },
            });
            return;
        }

        // 2) ì£¼ì˜/ì•„ì´ì»¨íƒ ë‚®ìŒ
        if (metrics.attention < attWarnTh || metrics.eyeContact < attWarnTh) {
            emit('attention', {
                type: 'attention',
                message: 'ë©´ì ‘ê´€ê³¼ì˜ ì•„ì´ì»¨íƒì„ ë” ìœ ì§€í•´ë³´ì„¸ìš”',
                level: 'warning',
                confidence: Math.max(metrics.attention, metrics.eyeContact),
                landmarks,
                metrics: {
                    attentionScore: metrics.attention,
                    eyeContactScore: metrics.eyeContact,
                    focusLevel: 'needs_improvement',
                },
            });
            return;
        }

        // 3) ìì‹ ê° ë‚®ìŒ
        if (metrics.confidence < confWarnTh) {
            emit('confidence', {
                type: 'confidence',
                message: 'ì¢€ ë” ìì‹ ê° ìˆëŠ” í‘œì •ì„ ì§€ì–´ë³´ì„¸ìš”',
                level: 'warning',
                confidence: metrics.confidence,
                landmarks,
                metrics: {
                    confidenceScore: metrics.confidence,
                    smileIntensity: metrics.smile,
                    overallPresence: 'needs_improvement',
                },
            });
            return;
        }

        // 4) ê¸ì • í”¼ë“œë°±
        if (metrics.confidence > confGoodTh) {
            emit('confidence', {
                type: 'confidence',
                message: 'ìì‹ ê° ìˆëŠ” í‘œì •ì´ ì¸ìƒì ì…ë‹ˆë‹¤!',
                level: 'excellent',
                confidence: metrics.confidence,
                landmarks,
                metrics: {
                    confidenceScore: metrics.confidence,
                    smileIntensity: metrics.smile,
                    overallPresence: 'good',
                },
            });
            return;
        }

        if (metrics.attention > 0.8 && metrics.eyeContact > 0.7) {
            emit('attention', {
                type: 'attention',
                message: 'í›Œë¥­í•œ ì§‘ì¤‘ë ¥ê³¼ ì•„ì´ì»¨íƒì„ ë³´ì—¬ì£¼ê³  ìˆìŠµë‹ˆë‹¤',
                level: 'excellent',
                confidence: (metrics.attention + metrics.eyeContact) / 2,
                landmarks,
                metrics: {
                    attentionScore: metrics.attention,
                    eyeContactScore: metrics.eyeContact,
                    focusLevel: 'high',
                },
            });
            return;
        }

        if (metrics.smile > 0.4) {
            emit('smile', {
                type: 'smile',
                message: 'ìì—°ìŠ¤ëŸ¬ìš´ ë¯¸ì†Œê°€ ì¢‹ì€ ì¸ìƒì„ ì¤ë‹ˆë‹¤',
                level: 'good',
                confidence: metrics.smile,
                landmarks,
                metrics: {
                    smileIntensity: metrics.smile,
                    facialExpression: 'positive',
                    warmth: 'high',
                },
            });
            return;
        }
    }

    private analyzeFrame() {
        if (!this.isAnalyzing || !this.videoRef.current || !this.canvasRef.current) {
            return;
        }

        const video = this.videoRef.current;
        const canvas = this.canvasRef.current;

        if (video.readyState !== 4 || !this.faceLandmarker) {
            if ('requestVideoFrameCallback' in video) {
                (video as any).requestVideoFrameCallback(() => this.analyzeFrame());
            } else {
                this.animationFrameId = requestAnimationFrame(() => this.analyzeFrame());
            }
            return;
        }

        canvas.width = video.videoWidth;
        canvas.height = video.videoHeight;

        const currentTime = Date.now();
        if (!this.hasLoggedInitialData) {
            this.logWebcamData(video, canvas);
            this.hasLoggedInitialData = true;
            this.lastLogTime = currentTime;
        }

        const timeSinceLastAnalysis = currentTime - this.lastAnalysisTime;
        if (timeSinceLastAnalysis < this.analysisInterval) {
            if ('requestVideoFrameCallback' in video) {
                (video as any).requestVideoFrameCallback(() => this.analyzeFrame());
            } else {
                this.animationFrameId = requestAnimationFrame(() => this.analyzeFrame());
            }
            return;
        }

        try {
            const results = this.faceLandmarker.detectForVideo(video, currentTime);
            if (results) {
                this.lastFaceResults = results;
                this.lastDetectionTime = currentTime;
                this.lastAnalysisTime = currentTime;
                this.analyzeFaceResults(results);
            }
        } catch (error) {
            console.error('âŒ Face Landmarker ê°ì§€ ì˜¤ë¥˜:', error);
        }

        if ('requestVideoFrameCallback' in video) {
            (video as any).requestVideoFrameCallback(() => this.analyzeFrame());
        } else {
            this.animationFrameId = requestAnimationFrame(() => this.analyzeFrame());
        }
    }

    private logWebcamData(video: HTMLVideoElement, canvas: HTMLCanvasElement) {
        const stream = video.srcObject as MediaStream;
        const tracks = stream?.getTracks() || [];
        console.log('ğŸ“Š MediaPipe Tasks Vision ì›¹ìº  ë°ì´í„° (ìµœì´ˆ 1íšŒ):', {
            timestamp: new Date().toISOString(),
            videoInfo: {
                width: video.videoWidth,
                height: video.videoHeight,
                readyState: video.readyState,
            },
            canvasInfo: { width: canvas.width, height: canvas.height },
            streamInfo: {
                id: stream?.id,
                active: stream?.active,
                tracksCount: tracks.length,
                tracks: tracks.map((t) => ({
                    kind: t.kind,
                    label: t.label,
                    readyState: t.readyState,
                })),
            },
        });
    }

    public async startAnalysis() {
        if (this.isAnalyzing) return;
        try {
            await this.initializeModels();
            await new Promise((r) => setTimeout(r, 300));
            this.isAnalyzing = true;
            this.lastDetectionTime = Date.now();
            this.lastAnalysisTime = Date.now();
            // ë¶„ì„ ë£¨í”„ ì‹œì‘(ìº˜ë¦¬ë¸Œë ˆì´ì…˜ê³¼ëŠ” ë³„ê°œ)
            // console.debug('MediaPipe ë¶„ì„ ë£¨í”„ ì‹œì‘ (5ì´ˆ ê°„ê²©)');
            this.analyzeFrame();
        } catch (error) {
            console.error('âŒ MediaPipe Tasks Vision ë¶„ì„ ì‹œì‘ ì‹¤íŒ¨:', error);
            throw error;
        }
    }

    public stopAnalysis() {
        this.isAnalyzing = false;
        if (this.animationFrameId) {
            cancelAnimationFrame(this.animationFrameId);
            this.animationFrameId = null;
        }
        console.log('â¹ï¸ MediaPipe Tasks Vision ë¶„ì„ ì¤‘ì§€');
    }

    public dispose() {
        console.log('ğŸ§¹ MediaPipe Tasks Vision ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì‹œì‘...');
        this.stopAnalysis();
        if (this.faceLandmarker) {
            this.faceLandmarker.close();
            this.faceLandmarker = null;
        }
        this.vision = null;
        this.isInitialized = false;
        console.log('âœ… MediaPipe Tasks Vision ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ');
    }

    // ===========================================
    // ì§‘ê³„ ë¡œì§ (ë²„í¼ â†’ ì„œë²„ ì „ì†¡ìš© í˜ì´ë¡œë“œ ë³€í™˜)
    // ===========================================
    private computeAggregate(samples: VisualSampleLite[]): VisualAggregatePayload {
        const n = samples.length;

        const toNumArr = (pick: (s: VisualSampleLite) => number | undefined) =>
            samples
                .map(pick)
                .filter((v): v is number => typeof v === 'number' && Number.isFinite(v));

        const mean = (arr: number[]) =>
            arr.length ? arr.reduce((a, b) => a + b, 0) / arr.length : null;
        const maxv = (arr: number[]) => (arr.length ? Math.max(...arr) : null);
        const std = (arr: number[]) => {
            if (arr.length === 0) return null;
            const m = mean(arr)!;
            const v = arr.reduce((a, x) => a + (x - m) * (x - m), 0) / arr.length;
            return Math.sqrt(v);
        };

        const confs = toNumArr((s) => s.detection.confidenceScore);
        const smiles = toNumArr((s) => s.detection.smileIntensity);
        const eyeContacts = toNumArr((s) => s.detection.eyeContact);
        const blinks = toNumArr((s) => s.detection.blinkProb);

        const presence = { good: 0, average: 0, needs_improvement: 0 };
        const level = { ok: 0, info: 0, warning: 0, critical: 0 as 0 };

        for (const s of samples) {
            const p = s.detection.overallPresence;
            if (p) presence[p]++;
            const l = s.detection.level;
            if (l) (level as any)[l]++;
        }

        const avgPt = (pick: (s: VisualSampleLite) => { x?: number; y?: number } | undefined) => {
            const xs: number[] = [];
            const ys: number[] = [];
            for (const s of samples) {
                const pt = pick(s);
                if (pt?.x != null && Number.isFinite(pt.x)) xs.push(pt.x);
                if (pt?.y != null && Number.isFinite(pt.y)) ys.push(pt.y);
            }
            if (!xs.length || !ys.length) return { x: null, y: null };
            return {
                x: xs.reduce((a, b) => a + b, 0) / xs.length,
                y: ys.reduce((a, b) => a + b, 0) / ys.length,
            };
        };

        const left = avgPt((s) => s.detection.landmarks.leftEye);
        const right = avgPt((s) => s.detection.landmarks.rightEye);
        const nose = avgPt((s) => s.detection.landmarks.nose);

        // ì‹œì„  ì•ˆì •ì„±: ì½” í¬ì¸íŠ¸ì˜ ì¢Œí‘œ ë³€ë™ í‘œì¤€í¸ì°¨ ê¸°ë°˜ (ì‘ì„ìˆ˜ë¡ ì•ˆì •)
        const noseXs = toNumArr((s) => s.detection.landmarks.nose?.x);
        const noseYs = toNumArr((s) => s.detection.landmarks.nose?.y);
        const sx = std(noseXs) ?? 0;
        const sy = std(noseYs) ?? 0;
        const motion = Math.sqrt(sx * sx + sy * sy);
        // 0~0.02 êµ¬ê°„ì„ 1~0ìœ¼ë¡œ ìŠ¤ì¼€ì¼ (ì˜ìƒ ì¢Œí‘œ ê¸°ì¤€ ê²½í—˜ê°’)
        const gazeStability = Math.max(0, Math.min(1, 1 - motion / 0.02));

        const ts = samples
            .map((s) => Date.parse(s.timestamp))
            .filter((v) => Number.isFinite(v)) as number[];

        return {
            sample_count: n,

            confidence_mean: mean(confs),
            confidence_max: maxv(confs),
            smile_mean: mean(smiles),
            smile_max: maxv(smiles),

            eye_contact_mean: mean(eyeContacts),
            blink_mean: mean(blinks),
            gaze_stability: Number.isFinite(gazeStability) ? gazeStability : null,

            presence_good: presence.good,
            presence_average: presence.average,
            presence_needs_improvement: presence.needs_improvement,

            level_ok: level.ok,
            level_info: level.info,
            level_warning: level.warning,
            level_critical: level.critical,

            left_eye_x_mean: left.x,
            left_eye_y_mean: left.y,
            right_eye_x_mean: right.x,
            right_eye_y_mean: right.y,
            nose_x_mean: nose.x,
            nose_y_mean: nose.y,

            started_at_ms: ts.length ? Math.min(...ts) : (this.questionStartedAt ?? null),
            ended_at_ms: ts.length ? Math.max(...ts) : this.questionStartedAt ? Date.now() : null,
        };
    }
}
