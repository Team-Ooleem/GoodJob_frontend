'use client';

interface DetectionResult {
    type: 'posture' | 'eye_contact' | 'smile' | 'gesture' | 'confidence' | 'attention' | 'stress';
    message: string;
    level: 'good' | 'warning' | 'excellent';
    confidence: number;
    landmarks: any;
    metrics: any;
}

type Presence = 'good' | 'average' | 'needs_improvement';
type LevelAgg = 'ok' | 'info' | 'warning' | 'critical';

// ì„œë²„ì— ë³´ë‚¼ ë¬¸í•­ ì§‘ê³„ í˜ì´ë¡œë“œ (Nestì˜ VisualAggregateDtoì™€ 1:1)
export type VisualAggregatePayload = {
    sample_count: number;

    confidence_mean: number | null;
    confidence_max: number | null;
    smile_mean: number | null;
    smile_max: number | null;

    presence_good: number;
    presence_average: number;
    presence_needs_improvement: number;

    level_ok: number;
    level_info: number;
    level_warning: number;
    level_critical: number;

    left_eye_x_mean: number | null;
    left_eye_y_mean: number | null;
    right_eye_x_mean: number | null;
    right_eye_y_mean: number | null;
    nose_x_mean: number | null;
    nose_y_mean: number | null;

    started_at_ms: number | null;
    ended_at_ms: number | null;

    // (ì„ íƒ) í´ë¼ì´ì–¸íŠ¸ í¸ì˜ë¥¼ ìœ„í•œ ë©”íƒ€
    _questionId?: string;
    _orderNo?: number;
    _text?: string;
};

// ë‚´ë¶€ ë²„í¼ì— ì €ì¥í•  ìƒ˜í”Œ ìŠ¤í‚¤ë§ˆ(ê²½ëŸ‰)
type VisualSampleLite = {
    timestamp: string; // ISO
    detection: {
        confidenceScore?: number;
        smileIntensity?: number;
        overallPresence?: Presence;
        level?: LevelAgg;
        landmarks: {
            leftEye?: { x: number; y: number };
            rightEye?: { x: number; y: number };
            nose?: { x: number; y: number };
        };
    };
};

export class RealMediaPipeAnalyzer {
    private videoRef: React.RefObject<HTMLVideoElement>;
    private canvasRef: React.RefObject<HTMLCanvasElement>;
    private onDetection: (data: DetectionResult) => void;

    private isAnalyzing = false;
    private animationFrameId: number | null = null;
    private lastLogTime = 0;
    private logInterval = 10000; // 10ì´ˆë§ˆë‹¤ ë¶„ì„ ê²°ê³¼ ë¡œê·¸ ì¶œë ¥
    private analysisInterval = 5000; // 5ì´ˆë§ˆë‹¤ ë¶„ì„ ì‹¤í–‰
    private lastAnalysisTime = 0;

    // MediaPipe Tasks Vision
    private faceLandmarker: any = null;
    private vision: any = null;
    private isInitialized = false;

    // ìƒíƒœ
    private lastFaceResults: any = null;
    private lastDetectionTime = 0;
    private lastAnalysisLogTime = 0;
    private hasLoggedInitialData = false;

    // â–¼ ì¶”ê°€: ë¬¸í•­ ë²„í¼ë§/ë©”íƒ€
    private currentQuestionId: string | null = null;
    private currentOrderNo: number | undefined;
    private currentQuestionText: string | undefined;
    private sampleBuffer: VisualSampleLite[] = [];
    private questionStartedAt: number | null = null;

    constructor(
        videoRef: React.RefObject<HTMLVideoElement>,
        canvasRef: React.RefObject<HTMLCanvasElement>,
        onDetection: (data: DetectionResult) => void,
    ) {
        this.videoRef = videoRef;
        this.canvasRef = canvasRef;
        this.onDetection = onDetection;
    }

    // ===========================
    // Public: ë¬¸í•­ ì‹œì‘/ì¢…ë£Œ API
    // ===========================
    public startQuestion(questionId: string, opts?: { orderNo?: number; text?: string }) {
        this.currentQuestionId = questionId;
        this.currentOrderNo = opts?.orderNo;
        this.currentQuestionText = opts?.text;
        this.sampleBuffer = [];
        this.questionStartedAt = Date.now();
        // ìµœì´ˆ í•œë‘ í”„ë ˆì„ì€ ë¶„ì„ ì´ì „ì¼ ìˆ˜ ìˆì–´ ìƒ˜í”Œ ì—†ì„ ìˆ˜ ìˆìŒ(OK)
        // ë³„ë„ ë¡œê·¸ë§Œ
        // console.log('[Mediapipe] startQuestion:', questionId, opts);
    }

    public endQuestion(): VisualAggregatePayload {
        const payload = this.computeAggregate(this.sampleBuffer);
        // ë©”íƒ€ í¬í•¨
        payload._questionId = this.currentQuestionId ?? undefined;
        payload._orderNo = this.currentOrderNo;
        payload._text = this.currentQuestionText;

        // ë‹¤ìŒ ë¬¸í•­ ìœ„í•´ ë¦¬ì…‹
        this.currentQuestionId = null;
        this.currentOrderNo = undefined;
        this.currentQuestionText = undefined;
        this.sampleBuffer = [];
        this.questionStartedAt = null;

        return payload;
    }

    // ===========================
    // MediaPipe ì´ˆê¸°í™”/ë£¨í”„
    // ===========================
    public async initializeModels() {
        if (this.isInitialized) {
            console.log('âš ï¸ MediaPipe ëª¨ë¸ì´ ì´ë¯¸ ì´ˆê¸°í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ì¤‘ë³µ ì‹¤í–‰ ë°©ì§€.');
            return;
        }
        try {
            console.log('ğŸš€ MediaPipe Tasks Vision ëª¨ë¸ ì´ˆê¸°í™” ì‹œì‘...');
            const { FaceLandmarker, FilesetResolver } = await import('@mediapipe/tasks-vision');
            this.vision = await FilesetResolver.forVisionTasks('/mediapipe/wasm');
            this.faceLandmarker = await FaceLandmarker.createFromOptions(this.vision, {
                baseOptions: { modelAssetPath: '/mediapipe/face_landmarker.task' },
                outputFaceBlendshapes: true,
                outputFacialTransformationMatrixes: false,
                runningMode: 'VIDEO',
                numFaces: 1,
            });
            this.isInitialized = true;
            console.log('âœ… MediaPipe Tasks Vision ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ');
        } catch (error) {
            console.error('âŒ MediaPipe Tasks Vision ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨:', error);
            this.isInitialized = false;
            throw error;
        }
    }

    private analyzeFaceResults(results: any) {
        const currentTime = Date.now();

        // 10ì´ˆë§ˆë‹¤ ìš”ì•½ ë¡œê·¸
        if (currentTime - this.lastAnalysisLogTime >= this.logInterval) {
            console.log('ğŸ‘ï¸ Face Landmarker ë¶„ì„ ê²°ê³¼ (10ì´ˆë§ˆë‹¤):', {
                timestamp: new Date().toISOString(),
                hasLandmarks: !!results.faceLandmarks,
                facesCount: results.faceLandmarks?.length || 0,
                hasBlendshapes: !!results.faceBlendshapes,
                blendshapesCount: results.faceBlendshapes?.length || 0,
                firstFaceLandmarks: results.faceLandmarks?.[0]
                    ? results.faceLandmarks[0].slice(0, 5)
                    : null,
            });
            this.lastAnalysisLogTime = currentTime;
        }

        if (!results.faceLandmarks || results.faceLandmarks.length === 0) return;

        const faceLandmarks = results.faceLandmarks[0];
        const faceBlendshapes = results.faceBlendshapes?.[0];

        const leftEyeCenter = faceLandmarks[159];
        const rightEyeCenter = faceLandmarks[386];
        const noseTip = faceLandmarks[1];

        const interviewMetrics = this.calculateInterviewMetrics(faceBlendshapes, faceLandmarks);

        // â–¼ ì‹¤ì‹œê°„ í”¼ë“œë°± (ê¸°ì¡´ ìœ ì§€)
        this.generateInterviewFeedback(interviewMetrics, leftEyeCenter, rightEyeCenter, noseTip);

        // â–¼ ì¶”ê°€: 5ì´ˆ ê°„ê²© ë¶„ì„ì‹œ **ë²„í¼ì— ìƒ˜í”Œ ê¸°ë¡** (ë¬¸í•­ ì§„í–‰ì¤‘ì¼ ë•Œë§Œ)
        if (this.currentQuestionId) {
            const presence: Presence =
                interviewMetrics.confidence > 0.7
                    ? 'good'
                    : interviewMetrics.confidence < 0.4
                      ? 'needs_improvement'
                      : 'average';

            // level ë§¤í•‘: warningë§Œ ê²½ê³ , ë‚˜ë¨¸ì§€ëŠ” okë¡œ ë‹¨ìˆœí™”(ì›í•˜ë©´ ë” ì •êµí™” ê°€ëŠ¥)
            const level: LevelAgg = interviewMetrics.stress > 0.6 ? 'warning' : 'ok';

            const sample: VisualSampleLite = {
                timestamp: new Date().toISOString(),
                detection: {
                    confidenceScore: interviewMetrics.confidence,
                    smileIntensity: interviewMetrics.smile,
                    overallPresence: presence,
                    level,
                    landmarks: {
                        leftEye: { x: leftEyeCenter.x, y: leftEyeCenter.y },
                        rightEye: { x: rightEyeCenter.x, y: rightEyeCenter.y },
                        nose: { x: noseTip.x, y: noseTip.y },
                    },
                },
            };
            this.sampleBuffer.push(sample);
        }
    }

    private calculateInterviewMetrics(faceBlendshapes: any, faceLandmarks: any) {
        const metrics = {
            confidence: 0,
            stress: 0,
            attention: 0,
            engagement: 0,
            eyeContact: 0,
            smile: 0,
            nervousness: 0,
            composure: 0,
        };
        if (!faceBlendshapes?.categories) return metrics;

        const blendshapes = faceBlendshapes.categories;

        const smileLeft =
            blendshapes.find((c: any) => c.categoryName === 'mouthSmileLeft')?.score || 0;
        const smileRight =
            blendshapes.find((c: any) => c.categoryName === 'mouthSmileRight')?.score || 0;
        const browInnerUp =
            blendshapes.find((c: any) => c.categoryName === 'browInnerUp')?.score || 0;
        const browOuterUpLeft =
            blendshapes.find((c: any) => c.categoryName === 'browOuterUpLeft')?.score || 0;
        const browOuterUpRight =
            blendshapes.find((c: any) => c.categoryName === 'browOuterUpRight')?.score || 0;

        metrics.smile = (smileLeft + smileRight) / 2;
        metrics.confidence =
            metrics.smile * 0.4 +
            (browOuterUpLeft + browOuterUpRight) * 0.3 +
            (1 - browInnerUp) * 0.3;

        const browDownLeft =
            blendshapes.find((c: any) => c.categoryName === 'browDownLeft')?.score || 0;
        const browDownRight =
            blendshapes.find((c: any) => c.categoryName === 'browDownRight')?.score || 0;
        const mouthPressLeft =
            blendshapes.find((c: any) => c.categoryName === 'mouthPressLeft')?.score || 0;
        const mouthPressRight =
            blendshapes.find((c: any) => c.categoryName === 'mouthPressRight')?.score || 0;

        metrics.stress = (browDownLeft + browDownRight + mouthPressLeft + mouthPressRight) / 4;
        metrics.nervousness = metrics.stress;

        const eyeBlinkLeft =
            blendshapes.find((c: any) => c.categoryName === 'eyeBlinkLeft')?.score || 0;
        const eyeBlinkRight =
            blendshapes.find((c: any) => c.categoryName === 'eyeBlinkRight')?.score || 0;
        const eyeLookDownLeft =
            blendshapes.find((c: any) => c.categoryName === 'eyeLookDownLeft')?.score || 0;
        const eyeLookDownRight =
            blendshapes.find((c: any) => c.categoryName === 'eyeLookDownRight')?.score || 0;

        metrics.attention = 1 - (eyeBlinkLeft + eyeBlinkRight) / 2;
        metrics.eyeContact = 1 - (eyeLookDownLeft + eyeLookDownRight) / 2;

        const cheekPuff = blendshapes.find((c: any) => c.categoryName === 'cheekPuff')?.score || 0;
        metrics.engagement = (metrics.smile + metrics.attention + (1 - cheekPuff)) / 3;
        metrics.composure = 1 - metrics.stress;

        return metrics;
    }

    private generateInterviewFeedback(metrics: any, leftEye: any, rightEye: any, nose: any) {
        const landmarks = {
            leftEye: { x: leftEye.x, y: leftEye.y },
            rightEye: { x: rightEye.x, y: rightEye.y },
            nose: { x: nose.x, y: nose.y },
        };

        // 1) ìŠ¤íŠ¸ë ˆìŠ¤ ë†’ìŒ
        if (metrics.stress > 0.6) {
            this.onDetection({
                type: 'stress',
                message: 'ê¸´ì¥ì„ í’€ê³  í¸ì•ˆí•œ ë§ˆìŒìœ¼ë¡œ ë‹µë³€í•´ë³´ì„¸ìš”',
                level: 'warning',
                confidence: 1 - metrics.stress,
                landmarks,
                metrics: {
                    stressLevel: metrics.stress,
                    nervousness: metrics.nervousness,
                    relaxationNeeded: true,
                },
            });
            return;
        }

        // 2) ì£¼ì˜/ì•„ì´ì»¨íƒ ë‚®ìŒ
        if (metrics.attention < 0.5 || metrics.eyeContact < 0.5) {
            this.onDetection({
                type: 'attention',
                message: 'ë©´ì ‘ê´€ê³¼ì˜ ì•„ì´ì»¨íƒì„ ë” ìœ ì§€í•´ë³´ì„¸ìš”',
                level: 'warning',
                confidence: Math.max(metrics.attention, metrics.eyeContact),
                landmarks,
                metrics: {
                    attentionScore: metrics.attention,
                    eyeContactScore: metrics.eyeContact,
                    focusLevel: 'needs_improvement',
                },
            });
            return;
        }

        // 3) ìì‹ ê° ë‚®ìŒ
        if (metrics.confidence < 0.4) {
            this.onDetection({
                type: 'confidence',
                message: 'ì¢€ ë” ìì‹ ê° ìˆëŠ” í‘œì •ì„ ì§€ì–´ë³´ì„¸ìš”',
                level: 'warning',
                confidence: metrics.confidence,
                landmarks,
                metrics: {
                    confidenceScore: metrics.confidence,
                    smileIntensity: metrics.smile,
                    overallPresence: 'needs_improvement',
                },
            });
            return;
        }

        // 4) ê¸ì • í”¼ë“œë°±
        if (metrics.confidence > 0.7) {
            this.onDetection({
                type: 'confidence',
                message: 'ìì‹ ê° ìˆëŠ” í‘œì •ì´ ì¸ìƒì ì…ë‹ˆë‹¤!',
                level: 'excellent',
                confidence: metrics.confidence,
                landmarks,
                metrics: {
                    confidenceScore: metrics.confidence,
                    smileIntensity: metrics.smile,
                    overallPresence: 'good',
                },
            });
            return;
        }

        if (metrics.attention > 0.8 && metrics.eyeContact > 0.7) {
            this.onDetection({
                type: 'attention',
                message: 'í›Œë¥­í•œ ì§‘ì¤‘ë ¥ê³¼ ì•„ì´ì»¨íƒì„ ë³´ì—¬ì£¼ê³  ìˆìŠµë‹ˆë‹¤',
                level: 'excellent',
                confidence: (metrics.attention + metrics.eyeContact) / 2,
                landmarks,
                metrics: {
                    attentionScore: metrics.attention,
                    eyeContactScore: metrics.eyeContact,
                    focusLevel: 'high',
                },
            });
            return;
        }

        if (metrics.smile > 0.4) {
            this.onDetection({
                type: 'smile',
                message: 'ìì—°ìŠ¤ëŸ¬ìš´ ë¯¸ì†Œê°€ ì¢‹ì€ ì¸ìƒì„ ì¤ë‹ˆë‹¤',
                level: 'good',
                confidence: metrics.smile,
                landmarks,
                metrics: {
                    smileIntensity: metrics.smile,
                    facialExpression: 'positive',
                    warmth: 'high',
                },
            });
            return;
        }
    }

    private analyzeFrame() {
        if (!this.isAnalyzing || !this.videoRef.current || !this.canvasRef.current) {
            return;
        }

        const video = this.videoRef.current;
        const canvas = this.canvasRef.current;

        if (video.readyState !== 4 || !this.faceLandmarker) {
            if ('requestVideoFrameCallback' in video) {
                (video as any).requestVideoFrameCallback(() => this.analyzeFrame());
            } else {
                this.animationFrameId = requestAnimationFrame(() => this.analyzeFrame());
            }
            return;
        }

        canvas.width = video.videoWidth;
        canvas.height = video.videoHeight;

        const currentTime = Date.now();
        if (!this.hasLoggedInitialData) {
            this.logWebcamData(video, canvas);
            this.hasLoggedInitialData = true;
            this.lastLogTime = currentTime;
        }

        const timeSinceLastAnalysis = currentTime - this.lastAnalysisTime;
        if (timeSinceLastAnalysis < this.analysisInterval) {
            if ('requestVideoFrameCallback' in video) {
                (video as any).requestVideoFrameCallback(() => this.analyzeFrame());
            } else {
                this.animationFrameId = requestAnimationFrame(() => this.analyzeFrame());
            }
            return;
        }

        try {
            const results = this.faceLandmarker.detectForVideo(video, currentTime);
            if (results) {
                this.lastFaceResults = results;
                this.lastDetectionTime = currentTime;
                this.lastAnalysisTime = currentTime;
                this.analyzeFaceResults(results);
            }
        } catch (error) {
            console.error('âŒ Face Landmarker ê°ì§€ ì˜¤ë¥˜:', error);
        }

        if ('requestVideoFrameCallback' in video) {
            (video as any).requestVideoFrameCallback(() => this.analyzeFrame());
        } else {
            this.animationFrameId = requestAnimationFrame(() => this.analyzeFrame());
        }
    }

    private logWebcamData(video: HTMLVideoElement, canvas: HTMLCanvasElement) {
        const stream = video.srcObject as MediaStream;
        const tracks = stream?.getTracks() || [];
        console.log('ğŸ“Š MediaPipe Tasks Vision ì›¹ìº  ë°ì´í„° (ìµœì´ˆ 1íšŒ):', {
            timestamp: new Date().toISOString(),
            videoInfo: {
                width: video.videoWidth,
                height: video.videoHeight,
                readyState: video.readyState,
            },
            canvasInfo: { width: canvas.width, height: canvas.height },
            streamInfo: {
                id: stream?.id,
                active: stream?.active,
                tracksCount: tracks.length,
                tracks: tracks.map((t) => ({
                    kind: t.kind,
                    label: t.label,
                    readyState: t.readyState,
                })),
            },
        });
    }

    public async startAnalysis() {
        if (this.isAnalyzing) return;
        try {
            await this.initializeModels();
            await new Promise((r) => setTimeout(r, 300));
            this.isAnalyzing = true;
            this.lastDetectionTime = Date.now();
            this.lastAnalysisTime = Date.now();
            console.log('ğŸ¯ MediaPipe Tasks Vision ë¶„ì„ ì‹œì‘ (5ì´ˆ ê°„ê²©)');
            this.analyzeFrame();
        } catch (error) {
            console.error('âŒ MediaPipe Tasks Vision ë¶„ì„ ì‹œì‘ ì‹¤íŒ¨:', error);
            throw error;
        }
    }

    public stopAnalysis() {
        this.isAnalyzing = false;
        if (this.animationFrameId) {
            cancelAnimationFrame(this.animationFrameId);
            this.animationFrameId = null;
        }
        console.log('â¹ï¸ MediaPipe Tasks Vision ë¶„ì„ ì¤‘ì§€');
    }

    public dispose() {
        console.log('ğŸ§¹ MediaPipe Tasks Vision ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì‹œì‘...');
        this.stopAnalysis();
        if (this.faceLandmarker) {
            this.faceLandmarker.close();
            this.faceLandmarker = null;
        }
        this.vision = null;
        this.isInitialized = false;
        console.log('âœ… MediaPipe Tasks Vision ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ');
    }

    // ===========================================
    // ì§‘ê³„ ë¡œì§ (ë²„í¼ â†’ ì„œë²„ ì „ì†¡ìš© í˜ì´ë¡œë“œ ë³€í™˜)
    // ===========================================
    private computeAggregate(samples: VisualSampleLite[]): VisualAggregatePayload {
        const n = samples.length;

        const toNumArr = (pick: (s: VisualSampleLite) => number | undefined) =>
            samples
                .map(pick)
                .filter((v): v is number => typeof v === 'number' && Number.isFinite(v));

        const mean = (arr: number[]) =>
            arr.length ? arr.reduce((a, b) => a + b, 0) / arr.length : null;
        const maxv = (arr: number[]) => (arr.length ? Math.max(...arr) : null);

        const confs = toNumArr((s) => s.detection.confidenceScore);
        const smiles = toNumArr((s) => s.detection.smileIntensity);

        const presence = { good: 0, average: 0, needs_improvement: 0 };
        const level = { ok: 0, info: 0, warning: 0, critical: 0 as 0 };

        for (const s of samples) {
            const p = s.detection.overallPresence;
            if (p) presence[p]++;
            const l = s.detection.level;
            if (l) (level as any)[l]++;
        }

        const avgPt = (pick: (s: VisualSampleLite) => { x?: number; y?: number } | undefined) => {
            const xs: number[] = [];
            const ys: number[] = [];
            for (const s of samples) {
                const pt = pick(s);
                if (pt?.x != null && Number.isFinite(pt.x)) xs.push(pt.x);
                if (pt?.y != null && Number.isFinite(pt.y)) ys.push(pt.y);
            }
            if (!xs.length || !ys.length) return { x: null, y: null };
            return {
                x: xs.reduce((a, b) => a + b, 0) / xs.length,
                y: ys.reduce((a, b) => a + b, 0) / ys.length,
            };
        };

        const left = avgPt((s) => s.detection.landmarks.leftEye);
        const right = avgPt((s) => s.detection.landmarks.rightEye);
        const nose = avgPt((s) => s.detection.landmarks.nose);

        const ts = samples
            .map((s) => Date.parse(s.timestamp))
            .filter((v) => Number.isFinite(v)) as number[];

        return {
            sample_count: n,

            confidence_mean: mean(confs),
            confidence_max: maxv(confs),
            smile_mean: mean(smiles),
            smile_max: maxv(smiles),

            presence_good: presence.good,
            presence_average: presence.average,
            presence_needs_improvement: presence.needs_improvement,

            level_ok: level.ok,
            level_info: level.info,
            level_warning: level.warning,
            level_critical: level.critical,

            left_eye_x_mean: left.x,
            left_eye_y_mean: left.y,
            right_eye_x_mean: right.x,
            right_eye_y_mean: right.y,
            nose_x_mean: nose.x,
            nose_y_mean: nose.y,

            started_at_ms: ts.length ? Math.min(...ts) : (this.questionStartedAt ?? null),
            ended_at_ms: ts.length ? Math.max(...ts) : this.questionStartedAt ? Date.now() : null,
        };
    }
}
