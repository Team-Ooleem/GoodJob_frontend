'use client';

import { useState, useEffect, useRef } from 'react';
import { Webcam, WebcamHandle, Question, Avatar } from '../_components';
import { message } from 'antd';
import { useInterviewAnalysis } from '@/hooks/use-interview-analysis';
import { blobToBase64, resampleTo16kHzMonoWav } from '@/utils/audio';
import axios from 'axios';
import { api } from '@/apis/api'; // ê²½ë¡œëŠ” ì‹¤ì œ ìœ„ì¹˜ì— ë§ê²Œ ì¡°ì •
import { AI_API_BASE, API_BASE_URL } from '@/constants/config';
import { speakSync, type SpeakSyncResponse } from '@/apis/avatar-api';

// ===== ì¶”ê°€: WAV ë ˆì½”ë” ìœ í‹¸ =====
class WavRecorder {
    private audioCtx: AudioContext | null = null;
    private stream: MediaStream | null = null;
    private source: MediaStreamAudioSourceNode | null = null;
    private processor: ScriptProcessorNode | null = null;
    private buffers: Float32Array[] = [];
    private recording = false;
    private stopped = false; // â¬…ï¸ ì¶”ê°€: ì¤‘ë³µ stop ë°©ì§€ í”Œë˜ê·¸

    async start() {
        // ì´ë¯¸ ì¼œì ¸ìˆë‹¤ë©´ ë¬´ì‹œ (í˜¹ì€ ë¨¼ì € stop() í˜¸ì¶œ)
        if (this.recording) return;
        this.stream = await navigator.mediaDevices.getUserMedia({ audio: true });
        this.audioCtx = new (window.AudioContext || (window as any).webkitAudioContext)();
        this.source = this.audioCtx.createMediaStreamSource(this.stream);
        this.processor = this.audioCtx.createScriptProcessor(4096, 1, 1);
        this.source.connect(this.processor);
        // ScriptProcessorëŠ” destinationì— ì—°ê²°í•´ì•¼ onaudioprocessê°€ íŠ¸ë¦¬ê±°ë¨
        this.processor.connect(this.audioCtx.destination);

        this.buffers = [];
        this.recording = true;
        this.stopped = false;

        this.processor.onaudioprocess = (e) => {
            if (!this.recording) return;
            const ch0 = e.inputBuffer.getChannelData(0);
            this.buffers.push(new Float32Array(ch0));
        };
    }

    async stop(): Promise<Blob> {
        // â¬‡ï¸ ì¤‘ë³µ í˜¸ì¶œ ë°©ì§€
        if (this.stopped) {
            // ì´ë¯¸ ë§Œë“¤ì–´ë‘” ë§ˆì§€ë§‰ WAVê°€ ì—†ë‹¤ë©´ ìµœì†Œí•œì˜ ë¹ˆ WAVë¼ë„ ë°˜í™˜
            return this.encodeWAV(new Float32Array(0), this.audioCtx?.sampleRate || 44100);
        }
        this.stopped = true;
        this.recording = false;

        // ì•ˆì „í•˜ê²Œ ëŠê¸°
        try {
            if (this.processor) this.processor.onaudioprocess = null;
        } catch {}
        try {
            if (this.processor) this.processor.disconnect();
        } catch {}
        try {
            if (this.source) this.source.disconnect();
        } catch {}
        try {
            if (this.stream) this.stream.getTracks().forEach((t) => t.stop());
        } catch {}

        const sr = this.audioCtx?.sampleRate || 44100;
        const samples = this.merge(this.buffers);
        const wav = this.encodeWAV(samples, sr);

        // AudioContext ë‹«ê¸° (ë‹«íŒ ìƒíƒœë©´ skip)
        try {
            if (this.audioCtx && this.audioCtx.state !== 'closed') {
                await this.audioCtx.close();
            }
        } catch {
            // InvalidStateError ë“±ì€ ë¬´ì‹œ
        } finally {
            this.audioCtx = null;
            this.stream = null;
            this.source = null;
            this.processor = null;
            this.buffers = [];
        }

        return wav;
    }

    private merge(chunks: Float32Array[]) {
        const total = chunks.reduce((a, b) => a + b.length, 0);
        const out = new Float32Array(total);
        let off = 0;
        for (const c of chunks) {
            out.set(c, off);
            off += c.length;
        }
        return out;
    }

    private encodeWAV(samples: Float32Array, sampleRate: number) {
        const buffer = new ArrayBuffer(44 + samples.length * 2);
        const view = new DataView(buffer);

        const writeString = (off: number, str: string) => {
            for (let i = 0; i < str.length; i++) view.setUint8(off + i, str.charCodeAt(i));
        };
        const floatTo16 = (off: number, input: Float32Array) => {
            for (let i = 0; i < input.length; i++, off += 2) {
                let s = Math.max(-1, Math.min(1, input[i]));
                s = s < 0 ? s * 0x8000 : s * 0x7fff;
                view.setInt16(off, s, true);
            }
        };

        writeString(0, 'RIFF');
        view.setUint32(4, 36 + samples.length * 2, true);
        writeString(8, 'WAVE');
        writeString(12, 'fmt ');
        view.setUint32(16, 16, true);
        view.setUint16(20, 1, true); // PCM
        view.setUint16(22, 1, true); // mono
        view.setUint32(24, sampleRate, true);
        view.setUint32(28, sampleRate * 2, true);
        view.setUint16(32, 2, true);
        view.setUint16(34, 16, true);
        writeString(36, 'data');
        view.setUint32(40, samples.length * 2, true);
        floatTo16(44, samples);

        return new Blob([view], { type: 'audio/wav' });
    }
}

// Web Speech API íƒ€ì… ì •ì˜
declare global {
    interface Window {
        webkitSpeechRecognition: any;
    }
}

// ë”ë¯¸ ë©´ì ‘ ë°ì´í„°
const interviewData = {
    interviewer: {
        name: 'ì •ì¸í˜œ',
        title: 'ë¶€ì¥Â·ì„ì›',
    },
    questions: [
        'ë°±ì—”ë“œ ê°œë°œì—ì„œ ê°€ì¥ ì¤‘ìš”í•˜ê²Œ ìƒê°í•˜ëŠ” ê¸°ìˆ  ìŠ¤íƒì€ ë¬´ì—‡ì´ê³ , ê·¸ ì´ìœ ëŠ” ë¬´ì—‡ì¸ê°€ìš”?',
        'ëŒ€ìš©ëŸ‰ íŠ¸ë˜í”½ì„ ì²˜ë¦¬í•˜ê¸° ìœ„í•œ ì•„í‚¤í…ì²˜ ì„¤ê³„ ê²½í—˜ì´ ìˆë‚˜ìš”?',
        'ë°ì´í„°ë² ì´ìŠ¤ ì„±ëŠ¥ ìµœì í™”ë¥¼ ìœ„í•´ ì–´ë–¤ ë°©ë²•ë“¤ì„ ì‚¬ìš©í•´ë³´ì…¨ë‚˜ìš”?',
    ],
};

type QuestionDto = { id: string; text: string };

interface AudioFeatures {
    f0_mean: number;
    f0_std: number;
    f0_cv?: number;
    f0_std_semitone?: number;
    rms_std: number;
    rms_cv: number;
    rms_cv_voiced?: number;
    rms_db_std_voiced?: number;
    jitter_like: number;
    shimmer_like: number;
    silence_ratio: number;
    sr: number;
    voiced_ratio?: number;
    voiced_frames?: number;
    total_frames?: number;
    // Diagnostics
    voiced_prob_mean?: number;
    voiced_prob_median?: number;
    voiced_prob_p90?: number;
    voiced_flag_ratio?: number;
    voiced_prob_ge_025_ratio?: number;
    voiced_prob_ge_035_ratio?: number;
    f0_valid_ratio?: number;
    silence_ratio_db50?: number;
    voiced_ratio_speech?: number;
    speech_frames?: number | null;
}

interface InterviewSession {
    questionNumber: number;
    question: string;
    answer: string;
    timeSpent: number;
    detectionData: any[];
    timestamp: Date;
    // ===== ì¶”ê°€: ì˜¤ë””ì˜¤ ê´€ë ¨ í•„ë“œ =====
    audioUrl?: string; // í´ë¼ì´ì–¸íŠ¸ì—ì„œ ì¬ìƒìš©
    audioFeatures?: AudioFeatures; // ì„œë²„ ë¶„ì„ ê²°ê³¼
}

export default function AiInterviewSessionsPage() {
    const [currentQuestionIndex, setCurrentQuestionIndex] = useState(0);
    const [isRecording, setIsRecording] = useState(false);
    const [timeLeft, setTimeLeft] = useState(60);
    const [isSpeaking, setIsSpeaking] = useState(false);
    const [avatarVideoUrl, setAvatarVideoUrl] = useState<string | null>(null);
    const [sessions, setSessions] = useState<InterviewSession[]>([]);
    const [currentSession, setCurrentSession] = useState<InterviewSession | null>(null);
    const [detectionHistory, setDetectionHistory] = useState<any[]>([]);
    const [transcribedText, setTranscribedText] = useState('');
    const [qaList, setQaList] = useState<Array<{ question: string; answer: string }>>([]);
    // ë™ì  ì§ˆë¬¸ ëª©ë¡ (AI ìƒì„±)
    const [dynamicQuestions, setDynamicQuestions] = useState<QuestionDto[]>([]);
    // ì „ì²´ ë¬¸í•­ ìˆ˜(ê¸°ì¡´ ë”ë¯¸ì™€ ë™ì¼í•˜ê²Œ 3ë¡œ ìœ ì§€; í•„ìš” ì‹œ ì¡°ì •)
    const MAX_QUESTIONS = 2;

    const timerRef = useRef<NodeJS.Timeout | null>(null);
    const speakingTimerRef = useRef<NodeJS.Timeout | null>(null);
    const recognitionRef = useRef<any>(null);
    const completingRef = useRef(false); // ì™„ë£Œ ì²˜ë¦¬ ì¬ì§„ì… ê°€ë“œ
    const webcamRef = useRef<WebcamHandle>(null);
    const isSpeakingRef = useRef(false);
    const lastSpokenQuestionIdRef = useRef<string | null>(null);

    // ===== ì¶”ê°€: WAV ë ˆì½”ë” ì¸ìŠ¤í„´ìŠ¤ & ìµœì‹  ì˜¤ë””ì˜¤ Blob ì°¸ì¡° =====
    const recorderRef = useRef<WavRecorder | null>(null);
    const lastAudioBlobRef = useRef<Blob | null>(null);

    // ì»´í¬ë„ŒíŠ¸ ë‚´ë¶€: ì„¸ì…˜IDë¥¼ í•œ ë²ˆ ìƒì„±í•´ ìœ ì§€ (uuid ì—†ì–´ë„ OK)
    const sessionIdRef = useRef<string>(
        `sess_${Math.random().toString(36).slice(2, 10)}_${Date.now()}`,
    );
    const SESSION_ID = sessionIdRef.current as any as string;
    // // ì§ˆë¬¸IDëŠ” q1/q2/... í˜•íƒœë¡œ ì“¸ê²Œìš”.
    // const qid = `q${currentQuestionIndex + 1}`;
    // const qtext = interviewData.questions[currentQuestionIndex];
    // í˜„ì¬ ì§ˆë¬¸(ë™ì ). ì•„ì§ ë¡œë”© ì „ì´ë©´ undefined
    const currentQuestion = dynamicQuestions[currentQuestionIndex];
    // ì´ˆê¸° ë Œë”ì—ì„œ ë”ë¯¸ ì§ˆë¬¸ì´ í™”ë©´ì— ì ê¹ ë³´ì˜€ë‹¤ê°€ ë°”ë€ŒëŠ” ë¬¸ì œë¥¼ ë§‰ê¸° ìœ„í•´,
    // ë™ì  ì§ˆë¬¸ì´ ì¤€ë¹„ë˜ê¸° ì „ì—” ì§ˆë¬¸ í…ìŠ¤íŠ¸ë¥¼ ë¹„ì›Œë‘”ë‹¤.
    const qtext = currentQuestion?.text || '';
    // ì§‘ê³„/ë©”íŠ¸ë¦­ ì „ì†¡ì—ëŠ” q1/q2/... (ê¸°ì¡´ ê·œì¹™ ìœ ì§€)
    const aggQid = `q${currentQuestionIndex + 1}`;
    // ì›¹ìº  startQuestionì—ëŠ” ì‹¤ì œ ì§ˆë¬¸ IDë¥¼ ì „ë‹¬(ì—†ìœ¼ë©´ aggQid)
    const currentQuestionId = currentQuestion?.id ?? aggQid;

    // react-query hook
    const interviewAnalysisMutation = useInterviewAnalysis();

    // const AI_API_BASE = `${process.env.NEXT_PUBLIC_API_BASE_URL}/ai`;

    // ===== API í´ë¼ì´ì–¸íŠ¸ ìœ í‹¸ =====
    async function fetchFirstQuestion(): Promise<QuestionDto> {
        //if (!AI_API_BASE) throw new Error('NEXT_PUBLIC_API_BASE_URL ë¯¸ì„¤ì •');
        // select/page.tsxì—ì„œ ì €ì¥í•œ ì„ íƒ ì´ë ¥ì„œ/ì±„ìš©ê³µê³  URL í™œìš©
        type SelectedResume = {
            id: number;
            title?: string;
            position?: string;
            company?: string;
            createdAt?: string;
            experience?: string;
            skills?: string[];
        };

        let resumeSummary: string | null = null;
        try {
            const raw = sessionStorage.getItem('selectedResume');
            if (raw) {
                const r = JSON.parse(raw) as SelectedResume;
                const parts = [
                    r.title,
                    r.position ? `í¬ì§€ì…˜: ${r.position}` : undefined,
                    r.company ? `íšŒì‚¬: ${r.company}` : undefined,
                    r.experience ? `ê²½ë ¥: ${r.experience}` : undefined,
                    Array.isArray(r.skills) && r.skills.length
                        ? `ê¸°ìˆ : ${r.skills.join(', ')}`
                        : undefined,
                ].filter(Boolean) as string[];
                if (parts.length) {
                    resumeSummary = parts.join(' | ');
                }
            }
        } catch (e) {
            console.warn('selectedResume íŒŒì‹± ì‹¤íŒ¨:', e);
        }

        // í´ë°±: ì˜ˆì „ ë¡œì»¬ ì €ì¥ ìš”ì•½ ë˜ëŠ” ê¸°ë³¸ê°’
        if (!resumeSummary) {
            resumeSummary =
                localStorage.getItem('resumeSummary') ||
                'í”„ë¡ íŠ¸ì—”ë“œ ê²½ë ¥ 3ë…„, Next.js/React, í¬ë˜í”„í†¤ ì •ê¸€ (ë¶€íŠ¸ìº í”„) ìˆ˜ë£Œ, Pintos ìš´ì˜ì²´ì œ í”„ë¡œì íŠ¸ ìˆ˜í–‰ ê²½í—˜';
        }

        const jobPostUrl = sessionStorage.getItem('jobPostUrl') || undefined;
        const payload: any = { resumeSummary };
        if (jobPostUrl) payload.jobPostUrl = jobPostUrl;

        const res = await api.post(`ai/question`, payload, { timeout: 60000 });
        const data = res.data as { question: QuestionDto };
        return data.question;
    }

    async function fetchFollowup(original: QuestionDto, answer: string): Promise<QuestionDto> {
        //if (!AI_API_BASE) throw new Error('NEXT_PUBLIC_API_BASE_URL ë¯¸ì„¤ì •');
        const res = await api.post(
            `ai/followups`,
            { originalQuestion: original, answer },
            { timeout: 60000 },
        );
        const data = res.data as {
            followups: Array<{ id: string; parentId: string; text: string; reason: string }>;
        };
        return { id: data.followups[0].id, text: data.followups[0].text };
    }

    const [isCompleting, setIsCompleting] = useState(false); // (ì„ íƒ) UIì—ì„œ ë²„íŠ¼ ë¹„í™œì„±í™” ë“±ì— ì‚¬ìš©

    // ê¸°ì¡´: Web Speech API ì‚¬ìš© ì—¬ë¶€ í”Œë˜ê·¸ (ì›í•˜ë©´ true ìœ ì§€)
    const USE_LOCAL_INTERIM_CAPTIONS = true;

    //const STT_API_BASE = `${process.env.NEXT_PUBLIC_API_BASE_URL}/stt`;

    async function transcribeWithGoogleSTT(wavBlob: Blob): Promise<string> {
        //if (!STT_API_BASE) throw new Error('NEXT_PUBLIC_STT_API_BASE_URL ë¯¸ì„¤ì •');
        // (ê¶Œì¥) 16kHz ë¦¬ìƒ˜í”Œ í›„ ì „ì†¡ â€” ì´ë¯¸ êµ¬í˜„í•´ë‘” í•¨ìˆ˜ ì¬ì‚¬ìš©
        const wav16k = await resampleTo16kHzMonoWav(wavBlob);

        const form = new FormData();
        form.append('file', wav16k, 'answer.wav'); // í•„ë“œëª…ì€ ì„œë²„ì˜ FileInterceptor('file')ì™€ ë™ì¼í•´ì•¼ í•¨

        const res = await api.post(`stt/transcribe-file`, form, {
            timeout: 120000,
            // â—ï¸Content-Type ìˆ˜ë™ ì§€ì • ê¸ˆì§€(axiosê°€ boundary ìë™ ì„¤ì •)
            // headers: { 'Content-Type': 'multipart/form-data' }
        });

        const data = res.data as { success: boolean; result: { transcript: string } };
        if (!data?.success) throw new Error('STT ì‹¤íŒ¨');
        return data.result.transcript || '';
    }

    // Axios ì˜¤ë¥˜ì—ì„œ Blob ë³¸ë¬¸ì„ í…ìŠ¤íŠ¸ë¡œ ë””ì½”ë“œí•´ ìì„¸í•œ ì›ì¸ì„ ë¡œê¹…
    const logAxiosBlobError = async (ctx: string, err: any) => {
        try {
            const status = err?.response?.status;
            const headers = err?.response?.headers;
            const data = err?.response?.data;
            if (data instanceof Blob) {
                const ct = data.type || headers?.['content-type'] || '';
                if (ct.includes('application/json') || ct.includes('text/')) {
                    const text = await data.text();
                    console.error(`[${ctx}] ì„œë²„ ì˜¤ë¥˜ ë³¸ë¬¸(${status}):`, text);
                } else {
                    console.error(`[${ctx}] ì„œë²„ê°€ Blob(${status}, ${ct})ì„ ë°˜í™˜í–ˆìŠµë‹ˆë‹¤.`);
                }
            } else if (data) {
                console.error(`[${ctx}] ì˜¤ë¥˜ ì‘ë‹µ(${status}):`, data);
            } else {
                console.error(`[${ctx}] ì˜¤ë¥˜:`, err?.message || err);
            }
        } catch (e) {
            console.error(`[${ctx}] ì˜¤ë¥˜ ë³¸ë¬¸ ë””ì½”ë”© ì‹¤íŒ¨:`, e);
        }
    };

    const synthesizeSpeech = async (text: string): Promise<string> => {
        //const TTS_API_BASE = `${process.env.NEXT_PUBLIC_API_BASE_URL}/tts`;
        //if (!TTS_API_BASE) throw new Error('NEXT_PUBLIC_API_BASE_URL ë¯¸ì„¤ì •');

        try {
            const response = await api.post(
                `tts/synthesize`,
                {
                    // ìš°ì„  ìµœì†Œ í•„ë“œë¡œ ìš”ì²­ (ë°±ì—”ë“œ ê¸°ë³¸ê°’ ì‚¬ìš©)
                    text,
                    // ì–¸ì–´/ë³´ì´ìŠ¤ëŠ” ì„œë²„ê°€ ì§€ì›í•˜ì§€ ì•Šì„ ìˆ˜ ìˆì–´ ê¸°ë³¸ì€ ìƒëµ
                    // í•„ìš” ì‹œ í™˜ê²½ë³€ìˆ˜ë¡œ ë®ì–´ì“°ê¸°
                    ...(process.env.NEXT_PUBLIC_TTS_LANGUAGE_CODE
                        ? { languageCode: process.env.NEXT_PUBLIC_TTS_LANGUAGE_CODE }
                        : {}),
                    ...(process.env.NEXT_PUBLIC_TTS_VOICE_NAME
                        ? { voiceName: process.env.NEXT_PUBLIC_TTS_VOICE_NAME }
                        : { voiceName: 'ko-KR-Chirp3-HD-Charon' }),
                    ...(process.env.NEXT_PUBLIC_TTS_AUDIO_ENCODING
                        ? { audioEncoding: process.env.NEXT_PUBLIC_TTS_AUDIO_ENCODING }
                        : { audioEncoding: 'MP3' }),
                },
                {
                    timeout: 30000,
                    responseType: 'blob',
                    // ì¼ë¶€ ì„œë²„ì—ì„œ ì¿ í‚¤ê°€ í•„ìš” ì—†ë‹¤ë©´ í¬ë¡œìŠ¤ë„ë©”ì¸ ì´ìŠˆë¥¼ ì¤„ì´ê¸° ìœ„í•´ ë¹„í™œì„±í™” ê°€ëŠ¥
                    // withCredentials: false,
                },
            );

            const audioUrl = URL.createObjectURL(response.data);
            return audioUrl;
        } catch (error: any) {
            console.error('TTS ìš”ì²­ ì‹¤íŒ¨:', error);
            await logAxiosBlobError('TTS', error);

            // 400 ë“± ìœ íš¨ì„± ì‹¤íŒ¨ ì‹œ, ì•ˆì „í•œ í•œêµ­ì–´ í‘œì¤€ ë³´ì´ìŠ¤ë¡œ 1íšŒ ì¬ì‹œë„
            try {
                const fallbackVoice =
                    process.env.NEXT_PUBLIC_TTS_FALLBACK_VOICE_NAME || 'ko-KR-Standard-A';
                const response2 = await api.post(
                    `tts/synthesize`,
                    {
                        text,
                        languageCode: process.env.NEXT_PUBLIC_TTS_LANGUAGE_CODE || 'ko-KR',
                        voiceName: fallbackVoice,
                        audioEncoding: process.env.NEXT_PUBLIC_TTS_AUDIO_ENCODING || 'MP3',
                    },
                    { timeout: 30000, responseType: 'blob' },
                );
                const audioUrl2 = URL.createObjectURL(response2.data);
                console.warn('TTS ì¬ì‹œë„: í‘œì¤€ ë³´ì´ìŠ¤ë¡œ ì„±ê³µ', fallbackVoice);
                return audioUrl2;
            } catch (retryErr) {
                await logAxiosBlobError('TTS(retry)', retryErr);
                throw error; // ìµœì´ˆ ì˜¤ë¥˜ë¥¼ ìœ ì§€í•´ ìƒìœ„ì—ì„œ ì²˜ë¦¬
            }
        }
    };

    // ì§ˆë¬¸ ì½ê¸°: ë™ê¸°ì‹ ì•„ë°”íƒ€ ë¹„ë””ì˜¤ â†’ ì‹¤íŒ¨ ì‹œ TTS í´ë°±
    const speakQuestion = async (questionText: string, overrideQuestionId?: string) => {
        if (isSpeakingRef.current) {
            console.log('ë°œí™”ê°€ ì´ë¯¸ ì§„í–‰ ì¤‘ì…ë‹ˆë‹¤. ì¤‘ë³µ ìš”ì²­ ë¬´ì‹œ.');
            return;
        }

        const useTalkingAvatar = process.env.NEXT_PUBLIC_TALKING_AVATAR === 'true';
        const defaultAvatarId = process.env.NEXT_PUBLIC_DEFAULT_AVATAR_ID;

        isSpeakingRef.current = true;
        setIsSpeaking(true);
        // í˜„ì¬ ì§ˆë¬¸ì„ ë§ˆì§€ë§‰ ë°œí™” ëŒ€ìƒìœ¼ë¡œ ê¸°ë¡(ì¤‘ë³µ íŠ¸ë¦¬ê±° ë°©ì§€)
        try {
            lastSpokenQuestionIdRef.current = overrideQuestionId ?? currentQuestionId;
        } catch {}

        if (useTalkingAvatar && defaultAvatarId) {
            try {
                const res: SpeakSyncResponse = await speakSync({
                    avatarId: defaultAvatarId,
                    text: questionText,
                    resolution: 256,
                    stillMode: true,
                });
                if (res?.success) {
                    setAvatarVideoUrl(res.videoUrl);
                    // onEnded í•¸ë“¤ëŸ¬ì—ì„œ speakingRef í•´ì œ
                    return;
                }
                console.warn('speak-sync í´ë°± ë°œìƒ, TTSë¡œ ëŒ€ì²´');
            } catch (e) {
                console.warn('speak-sync ì‹¤íŒ¨, TTSë¡œ í´ë°±', e);
            }
        }

        try {
            const audioUrl = await synthesizeSpeech(questionText);
            const audio = new Audio(audioUrl);
            audio.onended = () => {
                setIsSpeaking(false);
                isSpeakingRef.current = false;
                URL.revokeObjectURL(audioUrl);
            };
            audio.onerror = () => {
                console.error('ì˜¤ë””ì˜¤ ì¬ìƒ ì‹¤íŒ¨');
                setIsSpeaking(false);
                isSpeakingRef.current = false;
                URL.revokeObjectURL(audioUrl);
            };
            await audio.play();
        } catch (error) {
            console.error('TTS ì²˜ë¦¬ ì‹¤íŒ¨:', error);
            setIsSpeaking(false);
            isSpeakingRef.current = false;
            // ìµœì¢… í´ë°±: ì‹œë®¬ë ˆì´ì…˜
            simulateAISpeaking(3000);
        }
    };

    // ì§ˆë¬¸ì´ ì¤€ë¹„ë˜ë©´ ìë™ìœ¼ë¡œ ì½ì–´ì£¼ëŠ” íŠ¸ë¦¬ê±° (ì¤‘ë³µ ë°©ì§€ ê°€ë“œ í¬í•¨)
    useEffect(() => {
        const autoSpeak = process.env.NEXT_PUBLIC_AUTO_SPEAK_ON_QUESTION_READY !== 'false';
        if (!autoSpeak) return;
        if (!currentQuestionId || !qtext) return;
        // ì²« ë Œë” ì‹œ ë”ë¯¸ ë¬¸í•­(qtext ê¸°ë³¸ê°’)ìœ¼ë¡œ ë°œí™”ë˜ëŠ” ë¬¸ì œ ë°©ì§€: ì‹¤ì œ ë™ì  ì§ˆë¬¸ì´ ë¡œë“œëœ ë’¤ì—ë§Œ ë™ì‘
        if (dynamicQuestions.length === 0) return;
        // ì²« ì§ˆë¬¸ì€ ëª…ì‹œì ìœ¼ë¡œë§Œ ë°œí™”: ìë™ ë°œí™”ëŠ” 2ë²ˆì§¸ ì§ˆë¬¸ë¶€í„°
        if (currentQuestionIndex === 0) return;
        if (lastSpokenQuestionIdRef.current === currentQuestionId) return;
        if (isSpeakingRef.current) return;

        // ë¹„ë™ê¸° í˜¸ì¶œ(ì‹¤íŒ¨ëŠ” ì½˜ì†” ê²½ê³ ë§Œ)
        (async () => {
            try {
                await speakQuestion(qtext);
            } catch (e) {
                console.warn('ìë™ ì§ˆë¬¸ ì½ê¸° ì‹¤íŒ¨:', e);
            }
        })();
        // eslint-disable-next-line react-hooks/exhaustive-deps
    }, [currentQuestionId, currentQuestionIndex]);

    // AIê°€ ë§í•˜ëŠ” ì‹œë®¬ë ˆì´ì…˜
    const simulateAISpeaking = (duration: number = 3000) => {
        setIsSpeaking(true);
        if (speakingTimerRef.current) {
            clearTimeout(speakingTimerRef.current);
        }
        speakingTimerRef.current = setTimeout(() => {
            setIsSpeaking(false);
        }, duration);
    };

    // íƒ€ì´ë¨¸ ì‹œì‘
    const startTimer = () => {
        if (timerRef.current) {
            clearInterval(timerRef.current);
        }

        timerRef.current = setInterval(() => {
            setTimeLeft((prev) => {
                if (prev <= 1) {
                    // ì‹œê°„ ì´ˆê³¼
                    handleTimeUp();
                    return 0;
                }
                return prev - 1;
            });
        }, 1000);
    };

    // íƒ€ì´ë¨¸ ì •ì§€
    const stopTimer = () => {
        if (timerRef.current) {
            clearInterval(timerRef.current);
            timerRef.current = null;
        }
    };

    // ì‹œê°„ ì´ˆê³¼ ì²˜ë¦¬
    const handleTimeUp = () => {
        if (completingRef.current) return; // â¬…ï¸ ì´ë¯¸ ì™„ë£Œ ì¤‘ì´ë©´ ë¬´ì‹œ
        stopTimer();
        setIsRecording(false);
        message.warning('ì‹œê°„ì´ ì´ˆê³¼ë˜ì—ˆìŠµë‹ˆë‹¤.');
        handleCompleteAnswer();
    };

    // Web Speech API ì´ˆê¸°í™”
    const initializeSpeechRecognition = () => {
        if (typeof window !== 'undefined' && 'webkitSpeechRecognition' in window) {
            const SpeechRecognition = window.webkitSpeechRecognition;
            const recognition = new SpeechRecognition();

            recognition.continuous = true;
            recognition.interimResults = true;
            recognition.lang = 'ko-KR';

            recognition.onstart = () => {
                console.log('ğŸ¤ ìŒì„± ì¸ì‹ ì‹œì‘');
            };

            recognition.onresult = (event: any) => {
                let finalTranscript = '';
                let interimTranscript = '';

                for (let i = event.resultIndex; i < event.results.length; i++) {
                    const transcript = event.results[i][0].transcript;
                    if (event.results[i].isFinal) {
                        finalTranscript += transcript;
                    } else {
                        interimTranscript += transcript;
                    }
                }

                setTranscribedText(finalTranscript + interimTranscript);
            };

            recognition.onerror = (event: any) => {
                console.error('ìŒì„± ì¸ì‹ ì˜¤ë¥˜:', event.error);
            };

            recognition.onend = () => {
                console.log('ğŸ¤ ìŒì„± ì¸ì‹ ì¢…ë£Œ');
            };

            recognitionRef.current = recognition;
            return recognition;
        } else {
            console.warn('ì´ ë¸Œë¼ìš°ì €ëŠ” ìŒì„± ì¸ì‹ì„ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.');
            return null;
        }
    };

    // ìŒì„± ì¸ì‹ ì‹œì‘
    const startSpeechRecognition = () => {
        if (recognitionRef.current) {
            recognitionRef.current.start();
        }
    };

    // ìŒì„± ì¸ì‹ ì¤‘ì§€
    const stopSpeechRecognition = () => {
        if (recognitionRef.current) {
            recognitionRef.current.stop();
        }
    };

    // ê¸°ì¡´: fetch ë²„ì „ analyzeAudioBlob
    // -> axios ë²„ì „ìœ¼ë¡œ êµì²´
    const analyzeAudioBlob = async (
        blob: Blob,
        filename = 'answer.wav',
    ): Promise<AudioFeatures> => {
        if (!AI_API_BASE) throw new Error('AI_API_BASE ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.');
        const form = new FormData();
        form.append('file', blob, filename);

        // axiosëŠ” ë¸Œë¼ìš°ì €ì—ì„œ multipart boundaryë¥¼ ìë™ ì„¤ì •í•©ë‹ˆë‹¤. Content-Type ìˆ˜ë™ ì§€ì • X
        const res = await axios.post(`${AI_API_BASE}/audio/analyze`, form, {
            timeout: 60000, // ì„ íƒ: íƒ€ì„ì•„ì›ƒ
            withCredentials: false, // CORS ì¿ í‚¤ ë¯¸ì‚¬ìš©ì´ë©´ false (ê¸°ë³¸ê°’)
            // headers: { 'Content-Type': 'multipart/form-data' } // ì§ì ‘ ì§€ì •í•˜ì§€ ì•ŠëŠ” ê²ƒì„ ê¶Œì¥
        });
        return res.data?.features as AudioFeatures;
    };

    // ChatGPT API í˜¸ì¶œ í˜•ì‹ìœ¼ë¡œ ë°ì´í„° ë³€í™˜
    const formatMessagesForChatGPT = (qaList: Array<{ question: string; answer: string }>) => {
        const interviewContext = `
ë©´ì ‘ê´€: ${interviewData.interviewer.name} (${interviewData.interviewer.title})

ë©´ì ‘ ì§ˆë¬¸ê³¼ ë‹µë³€:
${qaList
    .map(
        (qa, index) => `
ì§ˆë¬¸ ${index + 1}: ${qa.question}
ë‹µë³€ ${index + 1}: ${qa.answer}
`,
    )
    .join('\n')}

ìœ„ì˜ ë©´ì ‘ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒê³¼ ê°™ì€ JSON í˜•ì‹ìœ¼ë¡œ ë¶„ì„í•´ì£¼ì„¸ìš”:

{
  "overall_score": 85,
  "detailed_scores": {
    "completeness": 8,
    "specificity": 7,
    "logic": 9,
    "impression": 8
  },
  "strengths": [
    "ê¸°ìˆ ì  ì§€ì‹ì´ íƒ„íƒ„í•¨",
    "ë…¼ë¦¬ì ì¸ ì‚¬ê³  ê³¼ì •",
    "êµ¬ì²´ì ì¸ ê²½í—˜ ì œì‹œ"
  ],
  "improvements": [
    "ë” êµ¬ì²´ì ì¸ ìˆ˜ì¹˜ì™€ ê²°ê³¼ ì œì‹œ í•„ìš”",
    "ì‹¤ë¬´ ê²½í—˜ì˜ ê¹Šì´ ë³´ì™„ í•„ìš”"
  ],
  "detailed_feedback": {
    "question_1": {
      "score": 8,
      "feedback": "ê¸°ìˆ  ìŠ¤íƒì— ëŒ€í•œ ê¸°ë³¸ì ì¸ ì´í•´ë„ëŠ” ìˆìœ¼ë‚˜, êµ¬ì²´ì ì¸ ê²½í—˜ ì‚¬ë¡€ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤."
    },
    "question_2": {
      "score": 7,
      "feedback": "ì•„í‚¤í…ì²˜ ì„¤ê³„ ê²½í—˜ì€ ìˆìœ¼ë‚˜, ëŒ€ìš©ëŸ‰ íŠ¸ë˜í”½ ì²˜ë¦¬ì— ëŒ€í•œ êµ¬ì²´ì ì¸ ìˆ˜ì¹˜ê°€ í•„ìš”í•©ë‹ˆë‹¤."
    },
    "question_3": {
      "score": 9,
      "feedback": "ë°ì´í„°ë² ì´ìŠ¤ ìµœì í™”ì— ëŒ€í•œ ê¹Šì´ ìˆëŠ” ì´í•´ë¥¼ ë³´ì—¬ì£¼ì—ˆìŠµë‹ˆë‹¤."
    }
  },
  "overall_evaluation": "ì „ë°˜ì ìœ¼ë¡œ ë°±ì—”ë“œ ê°œë°œì— ëŒ€í•œ ê¸°ë³¸ê¸°ëŠ” íƒ„íƒ„í•˜ë‚˜, ì‹¤ë¬´ ê²½í—˜ì„ ë°”íƒ•ìœ¼ë¡œ í•œ êµ¬ì²´ì ì¸ ì‚¬ë¡€ ì œì‹œê°€ í•„ìš”í•œ ìƒíƒœì…ë‹ˆë‹¤. ì§€ì†ì ì¸ í•™ìŠµê³¼ í”„ë¡œì íŠ¸ ê²½í—˜ì„ í†µí•´ ê¸°ìˆ ì  ê¹Šì´ë¥¼ ìŒ“ì•„ê°€ì‹œê¸¸ ê¶Œí•©ë‹ˆë‹¤.",
  "recommendations": [
    "ì‹¤ì œ í”„ë¡œì íŠ¸ì—ì„œì˜ ì„±ëŠ¥ ê°œì„  ì‚¬ë¡€ë¥¼ ì •ë¦¬í•´ë³´ì„¸ìš”",
    "ëŒ€ìš©ëŸ‰ íŠ¸ë˜í”½ ì²˜ë¦¬ ê²½í—˜ì„ ìŒ“ê¸° ìœ„í•œ ì‚¬ì´ë“œ í”„ë¡œì íŠ¸ë¥¼ ì¶”ì²œí•©ë‹ˆë‹¤",
    "ê¸°ìˆ  ë¸”ë¡œê·¸ ì‘ì„±ì„ í†µí•´ í•™ìŠµí•œ ë‚´ìš©ì„ ì •ë¦¬í•´ë³´ì„¸ìš”"
  ]
}

ë°˜ë“œì‹œ ìœ„ì˜ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•´ì£¼ì„¸ìš”. ë‹¤ë¥¸ í…ìŠ¤íŠ¸ëŠ” í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.
        `.trim();

        return {
            messages: [
                {
                    role: 'system' as const,
                    content:
                        'ë‹¹ì‹ ì€ ê²½í—˜ì´ í’ë¶€í•œ ë©´ì ‘ê´€ì…ë‹ˆë‹¤. ë©´ì ‘ìì˜ ë‹µë³€ì„ ê°ê´€ì ì´ê³  ê±´ì„¤ì ìœ¼ë¡œ ë¶„ì„í•˜ì—¬ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”.',
                },
                {
                    role: 'user' as const,
                    content: interviewContext,
                },
            ],
        };
    };

    // ë©´ì ‘ ì™„ë£Œ ì²˜ë¦¬ (API í˜¸ì¶œ í¬í•¨)
    const handleInterviewCompletion = async (finalSessions?: InterviewSession[]) => {
        // ìµœì‹  qaListë¥¼ ê°€ì ¸ì˜¤ê¸° ìœ„í•´ sessionsì—ì„œ ì¬êµ¬ì„±
        const sessionsToUse = finalSessions || sessions;
        const latestQAList = sessionsToUse.map((session) => ({
            question: session.question,
            answer: session.answer,
        }));

        // (ì„ íƒ) ì˜¤ë””ì˜¤ ì¢…í•© í‰ê·  ê°™ì€ ê°„ë‹¨ ìš”ì•½ ë§Œë“¤ê¸°
        const audioAgg = (() => {
            const feats = sessionsToUse
                .map((s) => s.audioFeatures)
                .filter(Boolean) as AudioFeatures[];
            const mean = (arr: number[]) =>
                arr.length ? arr.reduce((a, b) => a + b, 0) / arr.length : 0;
            const meanOf = (pick: (f: AudioFeatures) => number | undefined) =>
                mean(
                    feats
                        .map(pick)
                        .filter((v): v is number => typeof v === 'number' && isFinite(v)),
                );
            const safeCv = (f: AudioFeatures) =>
                typeof f.f0_cv === 'number' && isFinite(f.f0_cv) && f.f0_cv >= 0
                    ? f.f0_cv
                    : f.f0_mean > 0
                      ? (f.f0_std ?? 0) / f.f0_mean
                      : 0;
            const approxSemitoneStd = (cv: number) => {
                // ê·¼ì‚¬: ì„¸ë¯¸í†¤ í‘œì¤€í¸ì°¨ â‰ˆ 12 * log2(1 + CV)
                return 12 * Math.log2(1 + Math.max(0, cv));
            };
            return {
                f0_mean: mean(feats.map((f) => f.f0_mean)),
                f0_std: mean(feats.map((f) => f.f0_std)),
                // ì¢…í•©ì—ë„ CV/ì„¸ë¯¸í†¤ í‘œì¤€í¸ì°¨ë¥¼ í¬í•¨í•´ í†¤ ì ìˆ˜ ì•ˆì •í™”
                f0_cv: mean(feats.map((f) => safeCv(f))),
                f0_std_semitone: mean(feats.map((f) => approxSemitoneStd(safeCv(f)))),
                rms_cv: mean(feats.map((f) => f.rms_cv)),
                rms_cv_voiced: mean(
                    feats.map((f) =>
                        typeof f.rms_cv_voiced === 'number' ? f.rms_cv_voiced : f.rms_cv || 0,
                    ),
                ),
                rms_db_std_voiced: mean(
                    feats.map((f) =>
                        typeof f.rms_db_std_voiced === 'number' ? f.rms_db_std_voiced : 0,
                    ),
                ),
                jitter_like: mean(feats.map((f) => f.jitter_like)),
                shimmer_like: mean(feats.map((f) => f.shimmer_like)),
                silence_ratio: mean(feats.map((f) => f.silence_ratio)),
                silence_ratio_db50: meanOf((f) => f.silence_ratio_db50),
                voiced_ratio: mean(
                    feats.map((f) => (typeof f.voiced_ratio === 'number' ? f.voiced_ratio : 0)),
                ),
                voiced_ratio_speech: meanOf((f) => f.voiced_ratio_speech),
                // Diagnostics í‰ê· 
                voiced_prob_mean: meanOf((f) => f.voiced_prob_mean),
                voiced_prob_median: meanOf((f) => f.voiced_prob_median),
                voiced_prob_p90: meanOf((f) => f.voiced_prob_p90),
                voiced_flag_ratio: meanOf((f) => f.voiced_flag_ratio),
                voiced_prob_ge_025_ratio: meanOf((f) => f.voiced_prob_ge_025_ratio),
                voiced_prob_ge_035_ratio: meanOf((f) => f.voiced_prob_ge_035_ratio),
                f0_valid_ratio: meanOf((f) => f.f0_valid_ratio),
                speech_frames: meanOf((f) =>
                    typeof f.speech_frames === 'number' && isFinite(f.speech_frames)
                        ? f.speech_frames
                        : undefined,
                ),
            };
        })();

        // ê²°ê³¼ë¥¼ ë¡œì»¬ì— ë³´ê´€(ê²°ê³¼ í˜ì´ì§€ì—ì„œ í™œìš©)
        // ì£¼ì˜: audioUrl(data URL/Blob URL)ì€ ë§¤ìš° ì»¤ì„œ quotaë¥¼ ì´ˆê³¼í•  ìˆ˜ ìˆìŒ.
        // 1ì°¨ ì‹œë„: ì „ì²´ ì €ì¥, ì‹¤íŒ¨ ì‹œ audioUrl ì œê±° í›„ ì¶•ì†Œ ì €ì¥ìœ¼ë¡œ í´ë°±
        const audioPerQuestionFull = sessionsToUse.map((s) => ({
            questionNumber: s.questionNumber,
            question: s.question,
            audioFeatures: s.audioFeatures,
            audioUrl: s.audioUrl,
        }));
        try {
            localStorage.setItem('interviewAudioPerQuestion', JSON.stringify(audioPerQuestionFull));
        } catch (e) {
            // QuotaExceededError ë“± ë°œìƒ ì‹œ audioUrl ì œê±°í•˜ê³  ì¬ì‹œë„
            try {
                const reduced = audioPerQuestionFull.map(({ audioUrl, ...rest }) => rest);
                localStorage.setItem('interviewAudioPerQuestion', JSON.stringify(reduced));
                console.warn(
                    'localStorage quota exceeded: stored audioPerQuestion without audioUrl',
                );
            } catch (e2) {
                console.warn('Failed to store interviewAudioPerQuestion:', e2);
            }
        }
        try {
            localStorage.setItem('interviewAudioOverall', JSON.stringify(audioAgg));
        } catch (e) {
            console.warn('Failed to store interviewAudioOverall:', e);
        }

        try {
            const finalizeRes = await api.post(
                `/metrics/${SESSION_ID}/finalize`,
                {},
                { timeout: 10000 },
            );
            // ì‘ë‹µ í˜•ì‹: { ok: true, aggregate: { perQuestion: {...}, overall: {...} } }
            const visualAgg = finalizeRes.data?.aggregate;
            if (visualAgg) {
                // ê²°ê³¼ í˜ì´ì§€ì—ì„œ ì‚¬ìš©í•˜ë„ë¡ ë³´ê´€
                localStorage.setItem(
                    'interviewVisualPerQuestion',
                    JSON.stringify(visualAgg.perQuestion),
                );
                localStorage.setItem('interviewVisualOverall', JSON.stringify(visualAgg.overall));
            }
        } catch (e: any) {
            console.warn('ì„¸ì…˜ ì˜ìƒ ì§‘ê³„ finalize ì‹¤íŒ¨:', {
                url: `${API_BASE_URL}/metrics/${SESSION_ID}/finalize`,
                status: e?.response?.status,
                data: e?.response?.data,
            });
        }

        // ì§ˆë¬¸-ë‹µë³€ ë¦¬ìŠ¤íŠ¸ë¥¼ ì½˜ì†”ì— ì¶œë ¥
        console.log('ğŸ¯ ë©´ì ‘ ì™„ë£Œ - ì§ˆë¬¸ë‹µë³€ ë¦¬ìŠ¤íŠ¸:');
        console.log('=====================================');
        latestQAList.forEach((qa, index) => {
            console.log(`\nğŸ“ Q${index + 1}. ${qa.question}`);
            console.log(`ğŸ’¬ A${index + 1}. ${qa.answer}`);
            console.log('-------------------------------------');
        });
        console.log(`\nâœ… ì´ ${latestQAList.length}ê°œì˜ ì§ˆë¬¸ì— ë‹µë³€í–ˆìŠµë‹ˆë‹¤.`);
        console.log('=====================================');

        // ë°±ì—”ë“œ ë¦¬í¬íŠ¸ ë¶„ì„ í˜¸ì¶œë¡œ ì „í™˜
        message.loading('ë©´ì ‘ ê²°ê³¼ë¥¼ ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤...', 0);
        try {
            const res = await api.post(`/report/${SESSION_ID}/analyze`, { qa: latestQAList }, { timeout: 60000 });
            if (res.data?.success) {
                localStorage.setItem('interviewAnalysis', JSON.stringify(res.data.data));
            }
        } catch (error) {
            console.error('ë¦¬í¬íŠ¸ ë¶„ì„ í˜¸ì¶œ ì‹¤íŒ¨:', error);
        } finally {
            message.destroy();
            try { localStorage.setItem('interviewQA', JSON.stringify(latestQAList)); } catch {}
            setTimeout(() => { window.location.href = '/ai-interview/result'; }, 800);
        }
    };

    // ë‹µë³€ ì‹œì‘ - ì›ë˜ async ì—†ì—ˆëŠ”ë° ë°‘ì—ì„œ await ì“°ë©´ì„œ GPTê°€ ì¶”ê°€
    const handleStartAnswer = async () => {
        setIsRecording(true);
        setTimeLeft(60);
        setTranscribedText('');
        startTimer();

        // ìƒˆë¡œìš´ ì„¸ì…˜ ì‹œì‘
        const newSession: InterviewSession = {
            questionNumber: currentQuestionIndex + 1,
            question: qtext,
            answer: '',
            timeSpent: 0,
            detectionData: [],
            timestamp: new Date(),
        };
        setCurrentSession(newSession);

        // â–¼ ë¬¸í•­ ì‹œì‘: ì›¹ìº ì— ë¬¸í•­ ë©”íƒ€ ì „ë‹¬ (idëŠ” ì‹¤ì œ ì§ˆë¬¸ ID ì‚¬ìš©, ì—†ìœ¼ë©´ aggQid)
        webcamRef.current?.startQuestion(currentQuestionId, {
            orderNo: currentQuestionIndex + 1,
            text: qtext,
        });

        if (USE_LOCAL_INTERIM_CAPTIONS) {
            startSpeechRecognition(); // ìœ ì§€ ì‹œ
        }

        // WAV ë ˆì½”ë” ì‹œì‘
        try {
            recorderRef.current = new WavRecorder();
            await recorderRef.current.start();
        } catch (e) {
            console.error('ë§ˆì´í¬ ì ‘ê·¼ ì‹¤íŒ¨:', e);
            message.error('ë§ˆì´í¬ ì ‘ê·¼ ê¶Œí•œì„ í™•ì¸í•´ì£¼ì„¸ìš”.');
        }

        // AIê°€ ë§í•˜ëŠ” ì‹œë®¬ë ˆì´ì…˜
        simulateAISpeaking(2000);
    };

    // ë‹µë³€ ì™„ë£Œ
    const handleCompleteAnswer = async () => {
        // ì¬ì§„ì…/ì¤‘ë³µ í˜¸ì¶œ ê°€ë“œ (íƒ€ì´ë¨¸ ë§Œë£Œì™€ ë²„íŠ¼ í´ë¦­ì´ ê²¹ì¹  ìˆ˜ ìˆìŒ)
        if (completingRef.current) return;
        completingRef.current = true;

        let completedSession: InterviewSession | null = null;

        try {
            stopTimer();
            setIsRecording(false);

            // ìŒì„± ì¸ì‹ ì¤‘ì§€ëŠ” ì‹¤íŒ¨í•´ë„ ë¬´ì‹œ
            try {
                if (USE_LOCAL_INTERIM_CAPTIONS) stopSpeechRecognition();
            } catch {}

            // â–¼ ë¬¸í•­ ì¢…ë£Œ: ì§‘ê³„ ê²°ê³¼ ë°›ê¸° & ì„œë²„ë¡œ ì „ì†¡
            const agg = webcamRef.current?.endQuestion();
            if (agg) {
                try {
                    await api.post(`metrics/${SESSION_ID}/${aggQid}/aggregate`, agg, {
                        timeout: 10000,
                    });
                } catch (e) {
                    console.warn('ë¬¸í•­ ì˜ìƒ ì§‘ê³„ ì—…ë¡œë“œ ì‹¤íŒ¨:', e);
                }
            }

            // WAV ì •ì§€ â†’ ì—…ë¡œë“œ/ì „ì‚¬
            let audioUrl: string | undefined;
            let audioFeatures: AudioFeatures | undefined;
            let sttTranscript: string | undefined;

            try {
                // recorder ì¸ìŠ¤í„´ìŠ¤ë¥¼ ì§€ì—­ ë³€ìˆ˜ë¡œ ë³µì‚¬í•˜ê³ , ì¦‰ì‹œ refì—ì„œ ë–¼ì–´ ì¬ì‚¬ìš©ì„ ì°¨ë‹¨
                const rec = recorderRef.current;
                recorderRef.current = null;

                if (rec) {
                    const blob = await rec.stop(); // WavRecorder.stop()ì´ í•œ ë²ˆë§Œ í˜¸ì¶œë˜ë„ë¡ ë³´ì¥
                    lastAudioBlobRef.current = blob;
                    // blob: URLì€ ë‹¤ë¥¸ í˜ì´ì§€ë¡œ ì´ë™í•˜ë©´ ë¬´íš¨ê°€ ë˜ë¯€ë¡œ, data URLë¡œ ë³€í™˜í•´ ì €ì¥
                    try {
                        const b64 = await blobToBase64(blob);
                        audioUrl = `data:audio/wav;base64,${b64}`;
                    } catch {
                        // í´ë°±: ë™ì¼ íƒ­ì—ì„œëŠ” ì¬ìƒ ê°€ëŠ¥í•˜ë¯€ë¡œ blob: URLë„ í™•ë³´
                        audioUrl = URL.createObjectURL(blob);
                    }

                    // (ì„ íƒ) ì˜¤ë””ì˜¤ ë¶„ì„
                    try {
                        audioFeatures = await analyzeAudioBlob(
                            blob,
                            `q${currentQuestionIndex + 1}.wav`,
                        );
                    } catch (e) {
                        console.warn('ì˜¤ë””ì˜¤ ë¶„ì„ ì‹¤íŒ¨:', e);
                    }

                    // ë°±ì—”ë“œì— ì˜¤ë””ì˜¤ ì§€í‘œ ì—…ì„œíŠ¸(ì„œë²„ ë¦¬í¬íŠ¸ ê³„ì‚°ìš©)
                    if (audioFeatures) {
                        try {
                            await api.post(
                                `audio-metrics/${SESSION_ID}/${aggQid}`,
                                audioFeatures,
                                { timeout: 10000 },
                            );
                        } catch (e) {
                            console.warn('ì˜¤ë””ì˜¤ ì§€í‘œ ì—…ì„œíŠ¸ ì‹¤íŒ¨:', e);
                        }
                    }

                    // âœ… Google STT í˜¸ì¶œ (ìµœì¢… ë‹µë³€ í™•ì •)
                    message.loading('êµ¬ê¸€ STTë¡œ ë‹µë³€ì„ ì „ì‚¬ ì¤‘...', 0);
                    try {
                        sttTranscript = await transcribeWithGoogleSTT(blob);
                    } finally {
                        message.destroy(); // ë©”ì‹œì§€ëŠ” finallyì—ì„œ ì•ˆì „í•˜ê²Œ ì •ë¦¬
                    }
                }
            } catch (e) {
                message.destroy();
                console.error('ë…¹ìŒ/ì „ì‚¬ ì²˜ë¦¬ ì‹¤íŒ¨:', e);
                message.warning('ì „ì‚¬(STT)ì— ì‹¤íŒ¨í–ˆì–´ìš”. ë„¤íŠ¸ì›Œí¬ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.');
            }

            if (currentSession) {
                // ìµœì¢… ë‹µë³€ì€ STT > ì„ì‹œ ìë§‰ > ê¸°ë³¸ ë¬¸êµ¬ ìˆœìœ¼ë¡œ ê²°ì •
                const finalAnswer =
                    (sttTranscript && sttTranscript.trim()) ||
                    (transcribedText && transcribedText.trim()) ||
                    `ë‹µë³€ ${currentSession.questionNumber}ë²ˆ ì™„ë£Œ`;

                completedSession = {
                    ...currentSession,
                    answer: finalAnswer,
                    timeSpent: 60 - timeLeft,
                    audioUrl,
                    audioFeatures,
                };

                setSessions((prev) => [...prev, completedSession!]);
                setQaList((prev) => [
                    ...prev,
                    { question: currentSession.question, answer: finalAnswer },
                ]);
                setCurrentSession(null);

                // ===== ë¬¸í•­ ì¢…ë£Œ ì‹œ: STT ë‹µë³€ìœ¼ë¡œ ê¼¬ë¦¬ì§ˆë¬¸ ìƒì„± =====
                if (currentQuestionIndex < MAX_QUESTIONS - 1) {
                    try {
                        // í˜„ì¬ ì§ˆë¬¸ ê°ì²´(ë™ì ) ê¸°ì¤€
                        const original: QuestionDto = currentQuestion
                            ? { id: currentQuestion.id, text: currentQuestion.text }
                            : { id: aggQid, text: currentSession.question };
                        message.loading('ë‹¤ìŒ ì§ˆë¬¸ì„ ìƒì„± ì¤‘...', 0);
                        const nextQ = await fetchFollowup(original, finalAnswer);
                        setDynamicQuestions((prev) => [...prev, nextQ]);

                        // âœ… ì§ˆë¬¸ ìƒì„± ì§í›„ ë°”ë¡œ ë‹¤ìŒ ë‹¨ê³„ ì‹¤í–‰
                        setTimeout(async () => {
                            setCurrentQuestionIndex((prev) => prev + 1);
                            setTimeLeft(60);
                            // nextQë¥¼ ì§ì ‘ ì‚¬ìš© (ìƒíƒœ ì—…ë°ì´íŠ¸ë¥¼ ê¸°ë‹¤ë¦¬ì§€ ì•ŠìŒ)
                            if (nextQ?.text) {
                                await speakQuestion(nextQ.text, nextQ.id);
                            } else {
                                simulateAISpeaking(1500);
                            }
                        }, 1000);
                    } catch (e) {
                        console.warn('ê¼¬ë¦¬ì§ˆë¬¸ ìƒì„± ì‹¤íŒ¨. ë”ë¯¸ë¡œ í´ë°±:', e);
                        const fallback: QuestionDto = {
                            id: `fallback_${Date.now()}`,
                            text:
                                interviewData.questions[
                                    Math.min(
                                        currentQuestionIndex + 1,
                                        interviewData.questions.length - 1,
                                    )
                                ] ||
                                'ì´ì „ ë‹µë³€ì—ì„œ ìˆ˜ì¹˜/ì„±ê³¼ë¥¼ í™•ì¸í•  ìˆ˜ ìˆëŠ” ì‚¬ë¡€ë¥¼ í•˜ë‚˜ ì œì‹œí•´ ì£¼ì„¸ìš”.',
                        };
                        setDynamicQuestions((prev) => [...prev, fallback]);

                        // âœ… í´ë°± ì§ˆë¬¸ë„ ì§ì ‘ ì‚¬ìš©
                        setTimeout(async () => {
                            setCurrentQuestionIndex((prev) => prev + 1);
                            setTimeLeft(60);
                            if (fallback?.text) {
                                await speakQuestion(fallback.text, fallback.id);
                            } else {
                                simulateAISpeaking(1500);
                            }
                        }, 1000);
                    } finally {
                        try {
                            message.destroy();
                        } catch {}
                    }
                } else {
                    // ë§ˆì§€ë§‰ ì§ˆë¬¸ì¸ ê²½ìš°
                    message.success('ëª¨ë“  ë‹µë³€ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!');
                    setTimeout(() => {
                        handleInterviewCompletion([...sessions, completedSession!]);
                    }, 1000);
                }
            }

            setTranscribedText('');
        } finally {
            // ì¬ì§„ì… ê°€ëŠ¥ ìƒíƒœë¡œ ë³µêµ¬
            completingRef.current = false;
        }
    };

    // ì›¹ìº  ê°ì§€ ë°ì´í„° ì²˜ë¦¬
    const handleDetection = (data: any) => {
        setDetectionHistory((prev) => [...prev, data]);

        if (currentSession) {
            setCurrentSession((prev) =>
                prev
                    ? {
                          ...prev,
                          detectionData: [...prev.detectionData, data],
                      }
                    : null,
            );
        }
    };

    // ë§ˆìš´íŠ¸ ì‹œ "ì²« ì§ˆë¬¸" ì¤€ë¹„ + ìŒì„± ì¸ì‹ ì´ˆê¸°í™”
    useEffect(() => {
        try { localStorage.setItem('aiInterviewSessionId', SESSION_ID); } catch {}
        (async () => {
            try {
                const q = await fetchFirstQuestion();
                setDynamicQuestions([q]);

                // ì²« ì§ˆë¬¸ì„ TTSë¡œ ì½ê¸°
                if (q.text) {
                    await speakQuestion(q.text, q.id);
                }
            } catch (e) {
                console.warn('ì²« ì§ˆë¬¸ ìƒì„± ì‹¤íŒ¨. ë”ë¯¸ ì‚¬ìš©:', e);
                const fallbackQuestion = interviewData.questions[0];
                const fbId = 'q1';
                setDynamicQuestions([{ id: fbId, text: interviewData.questions[0] }]);
                if (fallbackQuestion) {
                    await speakQuestion(fallbackQuestion, fbId);
                }
            } finally {
                simulateAISpeaking(3000);
                initializeSpeechRecognition();
            }
        })();
        // eslint-disable-next-line react-hooks/exhaustive-deps
    }, []);

    // ì»´í¬ë„ŒíŠ¸ ì–¸ë§ˆìš´íŠ¸ ì‹œ ì •ë¦¬
    useEffect(() => {
        return () => {
            if (timerRef.current) {
                clearInterval(timerRef.current);
            }
            if (speakingTimerRef.current) {
                clearTimeout(speakingTimerRef.current);
            }
            if (recognitionRef.current) {
                recognitionRef.current.stop();
            }
        };
    }, []);

    return (
        <div className='w-screen h-screen flex flex-col justify-end items-center bg-gradient-to-br from-blue-100 via-indigo-50 to-purple-100 relative overflow-hidden'>
            {/* ì›¹ìº  */}
            <Webcam ref={webcamRef} css='absolute top-0 right-0' onDetection={handleDetection} />

            {/* AI ì•„ë°”íƒ€ */}
            <div>
                <Avatar
                    name={interviewData.interviewer.name}
                    title={interviewData.interviewer.title}
                    isSpeaking={isSpeaking}
                    videoUrl={avatarVideoUrl}
                    onEnded={() => {
                        setAvatarVideoUrl(null);
                        setIsSpeaking(false);
                        isSpeakingRef.current = false;
                    }}
                />
            </div>

            {/* ì§ˆë¬¸: ë™ì  ì§ˆë¬¸ì´ ì¤€ë¹„ëœ ê²½ìš°ì—ë§Œ ë Œë” */}
            {currentQuestion && (
                <Question
                    question={qtext}
                    questionNumber={currentQuestionIndex + 1}
                    totalQuestions={MAX_QUESTIONS}
                    isRecording={isRecording}
                    timeLeft={timeLeft}
                    onStartAnswer={handleStartAnswer}
                    onCompleteAnswer={handleCompleteAnswer}
                />
            )}

            {/* ë©´ì ‘ ì§„í–‰ ìƒíƒœ í‘œì‹œ */}
            <div className='absolute top-4 left-4 bg-white bg-opacity-90 rounded-lg p-4'>
                <div className='text-sm text-gray-600'>
                    <div>
                        ì§„í–‰ë¥ : {Math.round(((currentQuestionIndex + 1) / MAX_QUESTIONS) * 100)}%
                    </div>
                    <div>ì™„ë£Œëœ ì§ˆë¬¸: {sessions.length}</div>
                    <div>ê°ì§€ëœ í”¼ë“œë°±: {detectionHistory.length}</div>
                </div>
            </div>
        </div>
    );
}
