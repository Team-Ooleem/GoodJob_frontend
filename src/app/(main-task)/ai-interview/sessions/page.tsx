'use client';

import { useState, useEffect, useRef } from 'react';
import { Webcam, WebcamHandle, Question, Avatar } from '../_components';
import { message } from 'antd';
import { useInterviewAnalysis } from '@/hooks/use-interview-analysis';
import axios from 'axios';

// ===== ì¶”ê°€: WAV ë ˆì½”ë” ìœ í‹¸ =====
class WavRecorder {
    private audioCtx: AudioContext | null = null;
    private stream: MediaStream | null = null;
    private source: MediaStreamAudioSourceNode | null = null;
    private processor: ScriptProcessorNode | null = null;
    private buffers: Float32Array[] = [];
    private recording = false;

    async start() {
        this.stream = await navigator.mediaDevices.getUserMedia({ audio: true });
        this.audioCtx = new (window.AudioContext || (window as any).webkitAudioContext)();
        this.source = this.audioCtx.createMediaStreamSource(this.stream);
        this.processor = this.audioCtx.createScriptProcessor(4096, 1, 1);
        this.source.connect(this.processor);
        this.processor.connect(this.audioCtx.destination);
        this.buffers = [];
        this.recording = true;

        this.processor.onaudioprocess = (e) => {
            if (!this.recording) return;
            const ch0 = e.inputBuffer.getChannelData(0);
            this.buffers.push(new Float32Array(ch0));
        };
    }

    async stop(): Promise<Blob> {
        this.recording = false;
        if (this.processor) this.processor.disconnect();
        if (this.source) this.source.disconnect();
        if (this.stream) this.stream.getTracks().forEach((t) => t.stop());

        const sr = this.audioCtx?.sampleRate || 44100;
        const samples = this.merge(this.buffers);
        const wav = this.encodeWAV(samples, sr);
        if (this.audioCtx) await this.audioCtx.close();
        return wav;
    }

    private merge(chunks: Float32Array[]) {
        const total = chunks.reduce((a, b) => a + b.length, 0);
        const out = new Float32Array(total);
        let off = 0;
        for (const c of chunks) {
            out.set(c, off);
            off += c.length;
        }
        return out;
    }

    private encodeWAV(samples: Float32Array, sampleRate: number) {
        const buffer = new ArrayBuffer(44 + samples.length * 2);
        const view = new DataView(buffer);

        const writeString = (off: number, str: string) => {
            for (let i = 0; i < str.length; i++) view.setUint8(off + i, str.charCodeAt(i));
        };
        const floatTo16 = (off: number, input: Float32Array) => {
            for (let i = 0; i < input.length; i++, off += 2) {
                let s = Math.max(-1, Math.min(1, input[i]));
                s = s < 0 ? s * 0x8000 : s * 0x7fff;
                view.setInt16(off, s, true);
            }
        };

        writeString(0, 'RIFF');
        view.setUint32(4, 36 + samples.length * 2, true);
        writeString(8, 'WAVE');
        writeString(12, 'fmt ');
        view.setUint32(16, 16, true);
        view.setUint16(20, 1, true); // PCM
        view.setUint16(22, 1, true); // mono
        view.setUint32(24, sampleRate, true);
        view.setUint32(28, sampleRate * 2, true);
        view.setUint16(32, 2, true);
        view.setUint16(34, 16, true);
        writeString(36, 'data');
        view.setUint32(40, samples.length * 2, true);
        floatTo16(44, samples);

        return new Blob([view], { type: 'audio/wav' });
    }
}

// Web Speech API íƒ€ì… ì •ì˜
declare global {
    interface Window {
        webkitSpeechRecognition: any;
    }
}

// ë”ë¯¸ ë©´ì ‘ ë°ì´í„°
const interviewData = {
    interviewer: {
        name: 'ì •ì¸í˜œ',
        title: 'ë¶€ì¥Â·ì„ì›',
    },
    questions: [
        'ë°±ì—”ë“œ ê°œë°œì—ì„œ ê°€ì¥ ì¤‘ìš”í•˜ê²Œ ìƒê°í•˜ëŠ” ê¸°ìˆ  ìŠ¤íƒì€ ë¬´ì—‡ì´ê³ , ê·¸ ì´ìœ ëŠ” ë¬´ì—‡ì¸ê°€ìš”?',
        'ëŒ€ìš©ëŸ‰ íŠ¸ë˜í”½ì„ ì²˜ë¦¬í•˜ê¸° ìœ„í•œ ì•„í‚¤í…ì²˜ ì„¤ê³„ ê²½í—˜ì´ ìˆë‚˜ìš”?',
        'ë°ì´í„°ë² ì´ìŠ¤ ì„±ëŠ¥ ìµœì í™”ë¥¼ ìœ„í•´ ì–´ë–¤ ë°©ë²•ë“¤ì„ ì‚¬ìš©í•´ë³´ì…¨ë‚˜ìš”?',
    ],
};

interface AudioFeatures {
    f0_mean: number;
    f0_std: number;
    f0_cv?: number;
    f0_std_semitone?: number;
    rms_std: number;
    rms_cv: number;
    jitter_like: number;
    shimmer_like: number;
    silence_ratio: number;
    sr: number;
}

interface InterviewSession {
    questionNumber: number;
    question: string;
    answer: string;
    timeSpent: number;
    detectionData: any[];
    timestamp: Date;
    // ===== ì¶”ê°€: ì˜¤ë””ì˜¤ ê´€ë ¨ í•„ë“œ =====
    audioUrl?: string; // í´ë¼ì´ì–¸íŠ¸ì—ì„œ ì¬ìƒìš©
    audioFeatures?: AudioFeatures; // ì„œë²„ ë¶„ì„ ê²°ê³¼
}

export default function AiInterviewSessionsPage() {
    const [currentQuestionIndex, setCurrentQuestionIndex] = useState(0);
    const [isRecording, setIsRecording] = useState(false);
    const [timeLeft, setTimeLeft] = useState(60);
    const [isSpeaking, setIsSpeaking] = useState(false);
    const [sessions, setSessions] = useState<InterviewSession[]>([]);
    const [currentSession, setCurrentSession] = useState<InterviewSession | null>(null);
    const [detectionHistory, setDetectionHistory] = useState<any[]>([]);
    const [transcribedText, setTranscribedText] = useState('');
    const [qaList, setQaList] = useState<Array<{ question: string; answer: string }>>([]);

    const timerRef = useRef<NodeJS.Timeout | null>(null);
    const speakingTimerRef = useRef<NodeJS.Timeout | null>(null);
    const recognitionRef = useRef<any>(null);

    const webcamRef = useRef<WebcamHandle>(null);

    // ===== ì¶”ê°€: WAV ë ˆì½”ë” ì¸ìŠ¤í„´ìŠ¤ & ìµœì‹  ì˜¤ë””ì˜¤ Blob ì°¸ì¡° =====
    const recorderRef = useRef<WavRecorder | null>(null);
    const lastAudioBlobRef = useRef<Blob | null>(null);

    // ì»´í¬ë„ŒíŠ¸ ë‚´ë¶€: ì„¸ì…˜IDë¥¼ í•œ ë²ˆ ìƒì„±í•´ ìœ ì§€ (uuid ì—†ì–´ë„ OK)
    const sessionIdRef = useRef<string>(
        `sess_${Math.random().toString(36).slice(2, 10)}_${Date.now()}`,
    );
    const SESSION_ID = sessionIdRef.current as any as string;
    // ì§ˆë¬¸IDëŠ” q1/q2/... í˜•íƒœë¡œ ì“¸ê²Œìš”.
    const qid = `q${currentQuestionIndex + 1}`;
    const qtext = interviewData.questions[currentQuestionIndex];

    // react-query hook
    const interviewAnalysisMutation = useInterviewAnalysis();

    // AIê°€ ë§í•˜ëŠ” ì‹œë®¬ë ˆì´ì…˜
    const simulateAISpeaking = (duration: number = 3000) => {
        setIsSpeaking(true);
        if (speakingTimerRef.current) {
            clearTimeout(speakingTimerRef.current);
        }
        speakingTimerRef.current = setTimeout(() => {
            setIsSpeaking(false);
        }, duration);
    };

    // íƒ€ì´ë¨¸ ì‹œì‘
    const startTimer = () => {
        if (timerRef.current) {
            clearInterval(timerRef.current);
        }

        timerRef.current = setInterval(() => {
            setTimeLeft((prev) => {
                if (prev <= 1) {
                    // ì‹œê°„ ì´ˆê³¼
                    handleTimeUp();
                    return 0;
                }
                return prev - 1;
            });
        }, 1000);
    };

    // íƒ€ì´ë¨¸ ì •ì§€
    const stopTimer = () => {
        if (timerRef.current) {
            clearInterval(timerRef.current);
            timerRef.current = null;
        }
    };

    // ì‹œê°„ ì´ˆê³¼ ì²˜ë¦¬
    const handleTimeUp = () => {
        stopTimer();
        setIsRecording(false);
        message.warning('ì‹œê°„ì´ ì´ˆê³¼ë˜ì—ˆìŠµë‹ˆë‹¤.');
        handleCompleteAnswer();
    };

    // Web Speech API ì´ˆê¸°í™”
    const initializeSpeechRecognition = () => {
        if (typeof window !== 'undefined' && 'webkitSpeechRecognition' in window) {
            const SpeechRecognition = window.webkitSpeechRecognition;
            const recognition = new SpeechRecognition();

            recognition.continuous = true;
            recognition.interimResults = true;
            recognition.lang = 'ko-KR';

            recognition.onstart = () => {
                console.log('ğŸ¤ ìŒì„± ì¸ì‹ ì‹œì‘');
            };

            recognition.onresult = (event: any) => {
                let finalTranscript = '';
                let interimTranscript = '';

                for (let i = event.resultIndex; i < event.results.length; i++) {
                    const transcript = event.results[i][0].transcript;
                    if (event.results[i].isFinal) {
                        finalTranscript += transcript;
                    } else {
                        interimTranscript += transcript;
                    }
                }

                setTranscribedText(finalTranscript + interimTranscript);
            };

            recognition.onerror = (event: any) => {
                console.error('ìŒì„± ì¸ì‹ ì˜¤ë¥˜:', event.error);
            };

            recognition.onend = () => {
                console.log('ğŸ¤ ìŒì„± ì¸ì‹ ì¢…ë£Œ');
            };

            recognitionRef.current = recognition;
            return recognition;
        } else {
            console.warn('ì´ ë¸Œë¼ìš°ì €ëŠ” ìŒì„± ì¸ì‹ì„ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.');
            return null;
        }
    };

    // ìŒì„± ì¸ì‹ ì‹œì‘
    const startSpeechRecognition = () => {
        if (recognitionRef.current) {
            recognitionRef.current.start();
        }
    };

    // ìŒì„± ì¸ì‹ ì¤‘ì§€
    const stopSpeechRecognition = () => {
        if (recognitionRef.current) {
            recognitionRef.current.stop();
        }
    };

    // ê¸°ì¡´: fetch ë²„ì „ analyzeAudioBlob
    // -> axios ë²„ì „ìœ¼ë¡œ êµì²´
    const AUDIO_API_BASE = process.env.NEXT_PUBLIC_AUDIO_API_BASE; // ì˜ˆ: http://localhost:8081

    const analyzeAudioBlob = async (
        blob: Blob,
        filename = 'answer.wav',
    ): Promise<AudioFeatures> => {
        if (!AUDIO_API_BASE) throw new Error('NEXT_PUBLIC_AUDIO_API_BASEê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.');
        const form = new FormData();
        form.append('file', blob, filename);

        // axiosëŠ” ë¸Œë¼ìš°ì €ì—ì„œ multipart boundaryë¥¼ ìë™ ì„¤ì •í•©ë‹ˆë‹¤. Content-Type ìˆ˜ë™ ì§€ì • X
        const res = await axios.post(`${AUDIO_API_BASE}/audio/analyze`, form, {
            timeout: 60000, // ì„ íƒ: íƒ€ì„ì•„ì›ƒ
            withCredentials: false, // CORS ì¿ í‚¤ ë¯¸ì‚¬ìš©ì´ë©´ false (ê¸°ë³¸ê°’)
            // headers: { 'Content-Type': 'multipart/form-data' } // ì§ì ‘ ì§€ì •í•˜ì§€ ì•ŠëŠ” ê²ƒì„ ê¶Œì¥
        });
        return res.data?.features as AudioFeatures;
    };

    // ChatGPT API í˜¸ì¶œ í˜•ì‹ìœ¼ë¡œ ë°ì´í„° ë³€í™˜
    const formatMessagesForChatGPT = (qaList: Array<{ question: string; answer: string }>) => {
        const interviewContext = `
ë©´ì ‘ê´€: ${interviewData.interviewer.name} (${interviewData.interviewer.title})

ë©´ì ‘ ì§ˆë¬¸ê³¼ ë‹µë³€:
${qaList
    .map(
        (qa, index) => `
ì§ˆë¬¸ ${index + 1}: ${qa.question}
ë‹µë³€ ${index + 1}: ${qa.answer}
`,
    )
    .join('\n')}

ìœ„ì˜ ë©´ì ‘ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒê³¼ ê°™ì€ JSON í˜•ì‹ìœ¼ë¡œ ë¶„ì„í•´ì£¼ì„¸ìš”:

{
  "overall_score": 85,
  "detailed_scores": {
    "completeness": 8,
    "specificity": 7,
    "logic": 9,
    "impression": 8
  },
  "strengths": [
    "ê¸°ìˆ ì  ì§€ì‹ì´ íƒ„íƒ„í•¨",
    "ë…¼ë¦¬ì ì¸ ì‚¬ê³  ê³¼ì •",
    "êµ¬ì²´ì ì¸ ê²½í—˜ ì œì‹œ"
  ],
  "improvements": [
    "ë” êµ¬ì²´ì ì¸ ìˆ˜ì¹˜ì™€ ê²°ê³¼ ì œì‹œ í•„ìš”",
    "ì‹¤ë¬´ ê²½í—˜ì˜ ê¹Šì´ ë³´ì™„ í•„ìš”"
  ],
  "detailed_feedback": {
    "question_1": {
      "score": 8,
      "feedback": "ê¸°ìˆ  ìŠ¤íƒì— ëŒ€í•œ ê¸°ë³¸ì ì¸ ì´í•´ë„ëŠ” ìˆìœ¼ë‚˜, êµ¬ì²´ì ì¸ ê²½í—˜ ì‚¬ë¡€ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤."
    },
    "question_2": {
      "score": 7,
      "feedback": "ì•„í‚¤í…ì²˜ ì„¤ê³„ ê²½í—˜ì€ ìˆìœ¼ë‚˜, ëŒ€ìš©ëŸ‰ íŠ¸ë˜í”½ ì²˜ë¦¬ì— ëŒ€í•œ êµ¬ì²´ì ì¸ ìˆ˜ì¹˜ê°€ í•„ìš”í•©ë‹ˆë‹¤."
    },
    "question_3": {
      "score": 9,
      "feedback": "ë°ì´í„°ë² ì´ìŠ¤ ìµœì í™”ì— ëŒ€í•œ ê¹Šì´ ìˆëŠ” ì´í•´ë¥¼ ë³´ì—¬ì£¼ì—ˆìŠµë‹ˆë‹¤."
    }
  },
  "overall_evaluation": "ì „ë°˜ì ìœ¼ë¡œ ë°±ì—”ë“œ ê°œë°œì— ëŒ€í•œ ê¸°ë³¸ê¸°ëŠ” íƒ„íƒ„í•˜ë‚˜, ì‹¤ë¬´ ê²½í—˜ì„ ë°”íƒ•ìœ¼ë¡œ í•œ êµ¬ì²´ì ì¸ ì‚¬ë¡€ ì œì‹œê°€ í•„ìš”í•œ ìƒíƒœì…ë‹ˆë‹¤. ì§€ì†ì ì¸ í•™ìŠµê³¼ í”„ë¡œì íŠ¸ ê²½í—˜ì„ í†µí•´ ê¸°ìˆ ì  ê¹Šì´ë¥¼ ìŒ“ì•„ê°€ì‹œê¸¸ ê¶Œí•©ë‹ˆë‹¤.",
  "recommendations": [
    "ì‹¤ì œ í”„ë¡œì íŠ¸ì—ì„œì˜ ì„±ëŠ¥ ê°œì„  ì‚¬ë¡€ë¥¼ ì •ë¦¬í•´ë³´ì„¸ìš”",
    "ëŒ€ìš©ëŸ‰ íŠ¸ë˜í”½ ì²˜ë¦¬ ê²½í—˜ì„ ìŒ“ê¸° ìœ„í•œ ì‚¬ì´ë“œ í”„ë¡œì íŠ¸ë¥¼ ì¶”ì²œí•©ë‹ˆë‹¤",
    "ê¸°ìˆ  ë¸”ë¡œê·¸ ì‘ì„±ì„ í†µí•´ í•™ìŠµí•œ ë‚´ìš©ì„ ì •ë¦¬í•´ë³´ì„¸ìš”"
  ]
}

ë°˜ë“œì‹œ ìœ„ì˜ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•´ì£¼ì„¸ìš”. ë‹¤ë¥¸ í…ìŠ¤íŠ¸ëŠ” í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.
        `.trim();

        return {
            messages: [
                {
                    role: 'system' as const,
                    content:
                        'ë‹¹ì‹ ì€ ê²½í—˜ì´ í’ë¶€í•œ ë©´ì ‘ê´€ì…ë‹ˆë‹¤. ë©´ì ‘ìì˜ ë‹µë³€ì„ ê°ê´€ì ì´ê³  ê±´ì„¤ì ìœ¼ë¡œ ë¶„ì„í•˜ì—¬ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”.',
                },
                {
                    role: 'user' as const,
                    content: interviewContext,
                },
            ],
        };
    };

    // ë©´ì ‘ ì™„ë£Œ ì²˜ë¦¬ (API í˜¸ì¶œ í¬í•¨)
    const handleInterviewCompletion = async () => {
        // ìµœì‹  qaListë¥¼ ê°€ì ¸ì˜¤ê¸° ìœ„í•´ sessionsì—ì„œ ì¬êµ¬ì„±
        const latestQAList = sessions.map((session) => ({
            question: session.question,
            answer: session.answer,
        }));

        // (ì„ íƒ) ì˜¤ë””ì˜¤ ì¢…í•© í‰ê·  ê°™ì€ ê°„ë‹¨ ìš”ì•½ ë§Œë“¤ê¸°
        const audioAgg = (() => {
            const feats = sessions.map((s) => s.audioFeatures).filter(Boolean) as AudioFeatures[];
            const mean = (arr: number[]) =>
                arr.length ? arr.reduce((a, b) => a + b, 0) / arr.length : 0;
            return {
                f0_mean: mean(feats.map((f) => f.f0_mean)),
                f0_std: mean(feats.map((f) => f.f0_std)),
                rms_cv: mean(feats.map((f) => f.rms_cv)),
                jitter_like: mean(feats.map((f) => f.jitter_like)),
                shimmer_like: mean(feats.map((f) => f.shimmer_like)),
                silence_ratio: mean(feats.map((f) => f.silence_ratio)),
            };
        })();

        // ê²°ê³¼ë¥¼ ë¡œì»¬ì— ë³´ê´€(ê²°ê³¼ í˜ì´ì§€ì—ì„œ í™œìš©)
        localStorage.setItem(
            'interviewAudioPerQuestion',
            JSON.stringify(
                sessions.map((s) => ({
                    questionNumber: s.questionNumber,
                    question: s.question,
                    audioFeatures: s.audioFeatures,
                    audioUrl: s.audioUrl,
                })),
            ),
        );
        localStorage.setItem('interviewAudioOverall', JSON.stringify(audioAgg));

        try {
            const finalizeRes = await axios.post(
                `${process.env.NEXT_PUBLIC_API_BASE_URL}/api/metrics/${SESSION_ID}/finalize`,
                {},
                { timeout: 10000 },
            );
            // ì‘ë‹µ í˜•ì‹: { ok: true, aggregate: { perQuestion: {...}, overall: {...} } }
            const visualAgg = finalizeRes.data?.aggregate;
            if (visualAgg) {
                // ê²°ê³¼ í˜ì´ì§€ì—ì„œ ì‚¬ìš©í•˜ë„ë¡ ë³´ê´€
                localStorage.setItem(
                    'interviewVisualPerQuestion',
                    JSON.stringify(visualAgg.perQuestion),
                );
                localStorage.setItem('interviewVisualOverall', JSON.stringify(visualAgg.overall));
            }
        } catch (e: any) {
            console.warn('ì„¸ì…˜ ì˜ìƒ ì§‘ê³„ finalize ì‹¤íŒ¨:', {
                url: `${process.env.NEXT_PUBLIC_API_BASE_URL}/api/metrics/${SESSION_ID}/finalize`,
                status: e?.response?.status,
                data: e?.response?.data,
            });
        }

        // ì§ˆë¬¸-ë‹µë³€ ë¦¬ìŠ¤íŠ¸ë¥¼ ì½˜ì†”ì— ì¶œë ¥
        console.log('ğŸ¯ ë©´ì ‘ ì™„ë£Œ - ì§ˆë¬¸ë‹µë³€ ë¦¬ìŠ¤íŠ¸:');
        console.log('=====================================');
        latestQAList.forEach((qa, index) => {
            console.log(`\nğŸ“ Q${index + 1}. ${qa.question}`);
            console.log(`ğŸ’¬ A${index + 1}. ${qa.answer}`);
            console.log('-------------------------------------');
        });
        console.log(`\nâœ… ì´ ${latestQAList.length}ê°œì˜ ì§ˆë¬¸ì— ë‹µë³€í–ˆìŠµë‹ˆë‹¤.`);
        console.log('=====================================');

        // ë°±ì—”ë“œ API í˜¸ì¶œí•˜ì—¬ ë©´ì ‘ ë¶„ì„
        message.loading('ë©´ì ‘ ê²°ê³¼ë¥¼ ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤...', 0);

        try {
            // ìµœì‹  qaListë¡œ ë°ì´í„° í˜•ì‹ ë³€í™˜
            const requestData = formatMessagesForChatGPT(latestQAList);

            // ë°±ì—”ë“œ ê°œë°œììš© ìš”ì²­ ë°ì´í„° ë¡œê·¸ ì¶œë ¥
            console.log('ğŸš€ ë°±ì—”ë“œ API ìš”ì²­ ë°ì´í„°:');
            console.log('=====================================');
            console.log(
                'ğŸ“¡ API ì—”ë“œí¬ì¸íŠ¸:',
                `${process.env.NEXT_PUBLIC_API_BASE_URL}/interview/analyze`,
            );
            console.log('ğŸ“‹ ìš”ì²­ ë©”ì„œë“œ: POST');
            console.log('ğŸ“¦ ìš”ì²­ í—¤ë”:', {
                'Content-Type': 'application/json',
            });
            console.log('ğŸ“„ ìš”ì²­ ë°”ë””:', JSON.stringify(requestData, null, 2));
            console.log('=====================================');

            // ë°±ì—”ë“œ ê°œë°œììš© cURL ëª…ë ¹ì–´ ì˜ˆì‹œ
            console.log('ğŸ”§ ë°±ì—”ë“œ ê°œë°œììš© cURL ëª…ë ¹ì–´:');
            console.log('=====================================');
            console.log(
                `curl -X POST "${process.env.NEXT_PUBLIC_API_BASE_URL}/interview/analyze" \\`,
            );
            console.log(`  -H "Content-Type: application/json" \\`);
            console.log(`  -d '${JSON.stringify(requestData)}'`);
            console.log('=====================================');

            const analysisResult = await interviewAnalysisMutation.mutateAsync(requestData);
            message.destroy();

            if (analysisResult.success) {
                message.success('ë©´ì ‘ ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!');
                console.log('ğŸ¤– ë©´ì ‘ ë¶„ì„ ê²°ê³¼:', analysisResult);

                // ë¶„ì„ ê²°ê³¼ë¥¼ localStorageì— ì €ì¥ (ê²°ê³¼ í˜ì´ì§€ì—ì„œ ì‚¬ìš©)
                localStorage.setItem('interviewAnalysis', JSON.stringify(analysisResult.data));
                localStorage.setItem('interviewQA', JSON.stringify(latestQAList));
            } else {
                console.error('âŒ API ì‘ë‹µ ì‹¤íŒ¨:', analysisResult.error);
                // ì‹¤íŒ¨í•´ë„ localStorageì— ì‹¤íŒ¨ ì •ë³´ ì €ì¥
                localStorage.setItem(
                    'interviewAnalysis',
                    JSON.stringify({
                        error: true,
                        message: analysisResult.error || 'ë©´ì ‘ ë¶„ì„ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.',
                    }),
                );
                localStorage.setItem('interviewQA', JSON.stringify(latestQAList));
            }

            // ì„±ê³µ/ì‹¤íŒ¨ ê´€ê³„ì—†ì´ ê²°ê³¼ í˜ì´ì§€ë¡œ ì´ë™
            setTimeout(() => {
                window.location.href = '/ai-interview/result';
            }, 20000); // í…ŒìŠ¤íŠ¸ ìš©ë„ë¡œ ì¼ë‹¨ 20ì´ˆ, ì›ë˜ 2ì´ˆì˜€ìŒ
        } catch (error) {
            message.destroy();
            console.error('âŒ ë©´ì ‘ ë¶„ì„ API í˜¸ì¶œ ì˜¤ë¥˜:', error);

            // API í˜¸ì¶œ ì‹¤íŒ¨ ì‹œì—ë„ localStorageì— ì‹¤íŒ¨ ì •ë³´ ì €ì¥
            localStorage.setItem(
                'interviewAnalysis',
                JSON.stringify({
                    error: true,
                    message: 'API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë„¤íŠ¸ì›Œí¬ ì—°ê²°ì„ í™•ì¸í•´ì£¼ì„¸ìš”.',
                }),
            );
            localStorage.setItem('interviewQA', JSON.stringify(latestQAList));

            // ì‹¤íŒ¨í•´ë„ ê²°ê³¼ í˜ì´ì§€ë¡œ ì´ë™
            setTimeout(() => {
                window.location.href = '/ai-interview/result';
            }, 20000); // í…ŒìŠ¤íŠ¸ ìš©ë„ë¡œ ì¼ë‹¨ 20ì´ˆ, ì›ë˜ 2ì´ˆì˜€ìŒ
        }
    };

    // ë‹µë³€ ì‹œì‘ - ì›ë˜ async ì—†ì—ˆëŠ”ë° ë°‘ì—ì„œ await ì“°ë©´ì„œ GPTê°€ ì¶”ê°€
    const handleStartAnswer = async () => {
        setIsRecording(true);
        setTimeLeft(60);
        setTranscribedText('');
        startTimer();

        // ìƒˆë¡œìš´ ì„¸ì…˜ ì‹œì‘
        const newSession: InterviewSession = {
            questionNumber: currentQuestionIndex + 1,
            question: interviewData.questions[currentQuestionIndex],
            answer: '',
            timeSpent: 0,
            detectionData: [],
            timestamp: new Date(),
        };
        setCurrentSession(newSession);

        // â–¼ ë¬¸í•­ ì‹œì‘: ì›¹ìº ì— ë¬¸í•­ ë©”íƒ€ ì „ë‹¬
        webcamRef.current?.startQuestion(qid, { orderNo: currentQuestionIndex + 1, text: qtext });

        // ì‹¤ì œ ìŒì„± ì¸ì‹ ì‹œì‘ (ê¸°ì¡´ ì„¤ì • ìœ ì§€)
        startSpeechRecognition();

        // WAV ë ˆì½”ë” ì‹œì‘
        try {
            recorderRef.current = new WavRecorder();
            await recorderRef.current.start();
        } catch (e) {
            console.error('ë§ˆì´í¬ ì ‘ê·¼ ì‹¤íŒ¨:', e);
            message.error('ë§ˆì´í¬ ì ‘ê·¼ ê¶Œí•œì„ í™•ì¸í•´ì£¼ì„¸ìš”.');
        }

        // AIê°€ ë§í•˜ëŠ” ì‹œë®¬ë ˆì´ì…˜
        simulateAISpeaking(2000);
    };

    // ë‹µë³€ ì™„ë£Œ
    const handleCompleteAnswer = async () => {
        stopTimer();
        setIsRecording(false);

        // ìŒì„± ì¸ì‹ ì¤‘ì§€
        stopSpeechRecognition();

        // â–¼ ë¬¸í•­ ì¢…ë£Œ: ì§‘ê³„ ê²°ê³¼ ë°›ê¸° & ì„œë²„ë¡œ ì „ì†¡
        const qid = `q${currentQuestionIndex + 1}`;
        const agg = webcamRef.current?.endQuestion();
        if (agg) {
            try {
                await axios.post(
                    `${process.env.NEXT_PUBLIC_API_BASE_URL}/api/metrics/${SESSION_ID}/${qid}/aggregate`,
                    agg,
                    { timeout: 10000 },
                );
            } catch (e) {
                console.warn('ë¬¸í•­ ì˜ìƒ ì§‘ê³„ ì—…ë¡œë“œ ì‹¤íŒ¨:', e);
            }
        }

        // WAV ì •ì§€ â†’ ì—…ë¡œë“œ ë¶„ì„
        let audioUrl: string | undefined;
        let audioFeatures: AudioFeatures | undefined;

        try {
            if (recorderRef.current) {
                const blob = await recorderRef.current.stop();
                lastAudioBlobRef.current = blob;
                audioUrl = URL.createObjectURL(blob);

                // ì˜¤ë””ì˜¤ ë¶„ì„ ì„œë²„ í˜¸ì¶œ
                audioFeatures = await analyzeAudioBlob(blob, `q${currentQuestionIndex + 1}.wav`);
            }
        } catch (e) {
            console.error('ì˜¤ë””ì˜¤ ë¶„ì„ ì‹¤íŒ¨:', e);
            message.warning('ì˜¤ë””ì˜¤ ë¶„ì„ì— ì‹¤íŒ¨í–ˆì–´ìš”. ë„¤íŠ¸ì›Œí¬ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.');
        } finally {
            recorderRef.current = null;
        }

        if (currentSession) {
            const finalAnswer = transcribedText || `ë‹µë³€ ${currentSession.questionNumber}ë²ˆ ì™„ë£Œ`;

            const completedSession = {
                ...currentSession,
                answer: finalAnswer,
                timeSpent: 60 - timeLeft,
                audioUrl,
                audioFeatures,
            };

            setSessions((prev) => [...prev, completedSession]);

            // ì§ˆë¬¸-ë‹µë³€ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
            setQaList((prev) => [
                ...prev,
                {
                    question: currentSession.question,
                    answer: finalAnswer,
                },
            ]);

            setCurrentSession(null);
        }

        // ìŒì„± ì¸ì‹ í…ìŠ¤íŠ¸ ì´ˆê¸°í™”
        setTranscribedText('');

        // ë‹¤ìŒ ì§ˆë¬¸ìœ¼ë¡œ ì´ë™ ë˜ëŠ” ë©´ì ‘ ì™„ë£Œ
        if (currentQuestionIndex < interviewData.questions.length - 1) {
            // ì¼ë°˜ ì§ˆë¬¸ ì™„ë£Œ - ë‹¤ìŒ ì§ˆë¬¸ìœ¼ë¡œ ì´ë™
            setTimeout(() => {
                setCurrentQuestionIndex((prev) => prev + 1);
                setTimeLeft(60);
                simulateAISpeaking(1500);
            }, 2000);
        } else {
            // ë§ˆì§€ë§‰ ì§ˆë¬¸ ì™„ë£Œ - ë©´ì ‘ ì™„ë£Œ ì²˜ë¦¬
            message.success('ëª¨ë“  ë‹µë³€ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!');
            setTimeout(() => {
                handleInterviewCompletion();
            }, 2000);
        }
    };

    // ì›¹ìº  ê°ì§€ ë°ì´í„° ì²˜ë¦¬
    const handleDetection = (data: any) => {
        setDetectionHistory((prev) => [...prev, data]);

        if (currentSession) {
            setCurrentSession((prev) =>
                prev
                    ? {
                          ...prev,
                          detectionData: [...prev.detectionData, data],
                      }
                    : null,
            );
        }
    };

    // ì»´í¬ë„ŒíŠ¸ ë§ˆìš´íŠ¸ ì‹œ AI ì¸ì‚¬ë§ ë° Web Speech API ì´ˆê¸°í™”
    useEffect(() => {
        simulateAISpeaking(3000);
        initializeSpeechRecognition();
    }, []);

    // ì»´í¬ë„ŒíŠ¸ ì–¸ë§ˆìš´íŠ¸ ì‹œ ì •ë¦¬
    useEffect(() => {
        return () => {
            if (timerRef.current) {
                clearInterval(timerRef.current);
            }
            if (speakingTimerRef.current) {
                clearTimeout(speakingTimerRef.current);
            }
            if (recognitionRef.current) {
                recognitionRef.current.stop();
            }
        };
    }, []);

    return (
        <div className='w-screen h-screen flex flex-col justify-end items-center bg-gradient-to-br from-blue-100 via-indigo-50 to-purple-100 relative overflow-hidden'>
            {/* ì›¹ìº  */}
            <Webcam ref={webcamRef} css='absolute top-0 right-0' onDetection={handleDetection} />

            {/* AI ì•„ë°”íƒ€ */}
            <div>
                <Avatar
                    name={interviewData.interviewer.name}
                    title={interviewData.interviewer.title}
                    isSpeaking={isSpeaking}
                />
            </div>

            {/* ì§ˆë¬¸ */}
            {currentQuestionIndex < interviewData.questions.length && (
                <Question
                    question={interviewData.questions[currentQuestionIndex]}
                    questionNumber={currentQuestionIndex + 1}
                    totalQuestions={interviewData.questions.length}
                    isRecording={isRecording}
                    timeLeft={timeLeft}
                    onStartAnswer={handleStartAnswer}
                    onCompleteAnswer={handleCompleteAnswer}
                />
            )}

            {/* ë©´ì ‘ ì§„í–‰ ìƒíƒœ í‘œì‹œ */}
            <div className='absolute top-4 left-4 bg-white bg-opacity-90 rounded-lg p-4'>
                <div className='text-sm text-gray-600'>
                    <div>
                        ì§„í–‰ë¥ :{' '}
                        {Math.round(
                            ((currentQuestionIndex + 1) / interviewData.questions.length) * 100,
                        )}
                        %
                    </div>
                    <div>ì™„ë£Œëœ ì§ˆë¬¸: {sessions.length}</div>
                    <div>ê°ì§€ëœ í”¼ë“œë°±: {detectionHistory.length}</div>
                </div>
            </div>
        </div>
    );
}
