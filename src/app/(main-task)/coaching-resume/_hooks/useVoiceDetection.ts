'use client';

import { useCallback, useEffect, useRef, useState } from 'react';

interface UseVoiceDetection {
    isSpeaking: boolean;
    volumeLevel: number;
    startDetection: () => void;
    stopDetection: () => void;
}

interface VoiceDetectionOptions {
    threshold?: number; // ìŒì„± ê°ì§€ ì„ê³„ê°’ (0-1)
    smoothingFactor?: number; // ìŠ¤ë¬´ë”© íŒ©í„° (0-1)
    minSpeakingDuration?: number; // ìµœì†Œ ë§í•˜ê¸° ì§€ì† ì‹œê°„ (ms)
    debounceDelay?: number; // ë””ë°”ìš´ìŠ¤ ì§€ì—° ì‹œê°„ (ms)
}

export const useVoiceDetection = (
    stream: MediaStream | null,
    options: VoiceDetectionOptions = {},
): UseVoiceDetection => {
    const {
        threshold = 0.01, // ê¸°ë³¸ ì„ê³„ê°’
        smoothingFactor = 0.8, // ê¸°ë³¸ ìŠ¤ë¬´ë”© íŒ©í„°
        minSpeakingDuration = 100, // ìµœì†Œ 100ms
        debounceDelay = 50, // 50ms ë””ë°”ìš´ìŠ¤
    } = options;

    const [isSpeaking, setIsSpeaking] = useState(false);
    const [volumeLevel, setVolumeLevel] = useState(0);

    const audioContextRef = useRef<AudioContext | null>(null);
    const analyserRef = useRef<AnalyserNode | null>(null);
    const dataArrayRef = useRef<Uint8Array | null>(null);
    const animationFrameRef = useRef<number | null>(null);
    const smoothedVolumeRef = useRef(0);
    const speakingStartTimeRef = useRef<number | null>(null);
    const debounceTimeoutRef = useRef<NodeJS.Timeout | null>(null);
    const isSpeakingRef = useRef(false); // ì‹¤ì‹œê°„ ìƒíƒœ ì¶”ì ìš©

    // ì˜¤ë””ì˜¤ ì»¨í…ìŠ¤íŠ¸ ì´ˆê¸°í™”
    const initializeAudioContext = useCallback(() => {
        if (audioContextRef.current) return;

        try {
            audioContextRef.current = new (window.AudioContext ||
                (window as any).webkitAudioContext)();
            analyserRef.current = audioContextRef.current.createAnalyser();

            // ë¶„ì„ ì„¤ì •
            analyserRef.current.fftSize = 256;
            analyserRef.current.smoothingTimeConstant = smoothingFactor;

            // ë°ì´í„° ë°°ì—´ ì´ˆê¸°í™”
            const bufferLength = analyserRef.current.frequencyBinCount;
            dataArrayRef.current = new Uint8Array(bufferLength);
        } catch (error) {
            console.error('AudioContext ì´ˆê¸°í™” ì‹¤íŒ¨:', error);
        }
    }, [smoothingFactor]);

    // ìŒì„± ë ˆë²¨ ë¶„ì„ í•¨ìˆ˜
    const analyzeVolume = useCallback(() => {
        if (!analyserRef.current || !dataArrayRef.current) return;

        // ì£¼íŒŒìˆ˜ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        analyserRef.current.getByteFrequencyData(dataArrayRef.current);

        // í‰ê·  ë³¼ë¥¨ ê³„ì‚°
        let sum = 0;
        for (let i = 0; i < dataArrayRef.current.length; i++) {
            sum += dataArrayRef.current[i];
        }
        const average = sum / dataArrayRef.current.length;
        const normalizedVolume = average / 255; // 0-1 ë²”ìœ„ë¡œ ì •ê·œí™”

        // ìŠ¤ë¬´ë”© ì ìš©
        smoothedVolumeRef.current =
            smoothedVolumeRef.current * smoothingFactor + normalizedVolume * (1 - smoothingFactor);

        setVolumeLevel(smoothedVolumeRef.current);

        // ìŒì„± ê°ì§€ ë¡œì§
        const currentlySpeaking = smoothedVolumeRef.current > threshold;
        const now = Date.now();

        // ë””ë²„ê¹… ë¡œê·¸ (ê°œë°œ ì¤‘ì—ë§Œ ì‚¬ìš©)
        // if (process.env.NODE_ENV === 'development' && smoothedVolumeRef.current > 0.001) {
        //     console.log(
        //         `Volume: ${smoothedVolumeRef.current.toFixed(4)}, Threshold: ${threshold}, Currently Speaking: ${currentlySpeaking}, Is Speaking Ref: ${isSpeakingRef.current}, Is Speaking State: ${isSpeaking}`,
        //     );
        // }

        if (currentlySpeaking) {
            // ë§í•˜ê¸° ì¤‘ - ê¸°ì¡´ ë””ë°”ìš´ìŠ¤ íƒ€ì´ë¨¸ ì·¨ì†Œ
            if (debounceTimeoutRef.current) {
                clearTimeout(debounceTimeoutRef.current);
                debounceTimeoutRef.current = null;
            }

            if (!isSpeakingRef.current) {
                // ë§í•˜ê¸° ì‹œì‘ - ì¦‰ì‹œ ìƒíƒœ ì—…ë°ì´íŠ¸
                // if (process.env.NODE_ENV === 'development') {
                //     console.log('ğŸ¤ ë§í•˜ê¸° ì‹œì‘');
                // }
                isSpeakingRef.current = true;
                setIsSpeaking(true);
                speakingStartTimeRef.current = now;
            }
        } else {
            if (isSpeakingRef.current) {
                // ë§í•˜ê¸° ì¤‘ë‹¨ - ë””ë°”ìš´ìŠ¤ ì ìš© (ê¸°ì¡´ íƒ€ì´ë¨¸ê°€ ìˆìœ¼ë©´ ìœ ì§€)
                if (!debounceTimeoutRef.current) {
                    // if (process.env.NODE_ENV === 'development') {
                    //     console.log('â° ë§í•˜ê¸° ì¤‘ë‹¨ íƒ€ì´ë¨¸ ì‹œì‘ (50ms í›„)');
                    // }
                    debounceTimeoutRef.current = setTimeout(() => {
                        // if (process.env.NODE_ENV === 'development') {
                        //     console.log('ğŸ”‡ ë§í•˜ê¸° ì¤‘ë‹¨');
                        // }
                        isSpeakingRef.current = false;
                        setIsSpeaking(false);
                        speakingStartTimeRef.current = null;
                        debounceTimeoutRef.current = null;
                    }, debounceDelay);
                }
            }
        }

        // ë‹¤ìŒ í”„ë ˆì„ ì˜ˆì•½
        animationFrameRef.current = requestAnimationFrame(analyzeVolume);
    }, [threshold, smoothingFactor, minSpeakingDuration, debounceDelay]);

    // ìŒì„± ê°ì§€ ì‹œì‘
    const startDetection = useCallback(() => {
        if (!stream || audioContextRef.current?.state === 'closed') return;

        try {
            initializeAudioContext();

            if (audioContextRef.current && analyserRef.current) {
                // ì˜¤ë””ì˜¤ ìŠ¤íŠ¸ë¦¼ ì—°ê²°
                const source = audioContextRef.current.createMediaStreamSource(stream);
                source.connect(analyserRef.current);

                // ì˜¤ë””ì˜¤ ì»¨í…ìŠ¤íŠ¸ ì¬ê°œ (ì¼ì‹œì •ì§€ ìƒíƒœì¼ ìˆ˜ ìˆìŒ)
                if (audioContextRef.current.state === 'suspended') {
                    audioContextRef.current.resume();
                }

                // ë¶„ì„ ì‹œì‘
                analyzeVolume();
            }
        } catch (error) {
            console.error('ìŒì„± ê°ì§€ ì‹œì‘ ì‹¤íŒ¨:', error);
        }
    }, [stream, initializeAudioContext, analyzeVolume]);

    // ìŒì„± ê°ì§€ ì¤‘ì§€
    const stopDetection = useCallback(() => {
        if (animationFrameRef.current) {
            cancelAnimationFrame(animationFrameRef.current);
            animationFrameRef.current = null;
        }

        if (debounceTimeoutRef.current) {
            clearTimeout(debounceTimeoutRef.current);
            debounceTimeoutRef.current = null;
        }

        if (audioContextRef.current && audioContextRef.current.state !== 'closed') {
            audioContextRef.current.close();
            audioContextRef.current = null;
        }

        analyserRef.current = null;
        dataArrayRef.current = null;
        smoothedVolumeRef.current = 0;
        speakingStartTimeRef.current = null;
        isSpeakingRef.current = false;

        setIsSpeaking(false);
        setVolumeLevel(0);
    }, []);

    // isSpeakingRefì™€ isSpeaking ìƒíƒœ ë™ê¸°í™” (ê°œë°œ ì¤‘ì—ë§Œ ì‚¬ìš©)
    useEffect(() => {
        // if (process.env.NODE_ENV === 'development') {
        const interval = setInterval(() => {
            if (isSpeakingRef.current !== isSpeaking) {
                console.log(`ğŸ”„ ìƒíƒœ ë™ê¸°í™”: ${isSpeaking} â†’ ${isSpeakingRef.current}`);
                setIsSpeaking(isSpeakingRef.current);
            }
        }, 16); // 60fps

        return () => clearInterval(interval);
        // }
    }, [isSpeaking]);

    // ì»´í¬ë„ŒíŠ¸ ì–¸ë§ˆìš´íŠ¸ ì‹œ ì •ë¦¬
    useEffect(() => {
        return () => {
            stopDetection();
        };
    }, [stopDetection]);

    // ìŠ¤íŠ¸ë¦¼ì´ ë³€ê²½ë˜ë©´ ìë™ìœ¼ë¡œ ê°ì§€ ì‹œì‘
    useEffect(() => {
        if (stream) {
            startDetection();
        } else {
            stopDetection();
        }
    }, [stream, startDetection, stopDetection]);

    return {
        isSpeaking,
        volumeLevel,
        startDetection,
        stopDetection,
    };
};
